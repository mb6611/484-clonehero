{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor, Pop2PianoTokenizer\n",
    "from encoder import encode_plus\n",
    "import sys\n",
    "sys.path.append(\"./pop2piano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def crop_midi(midi, start_beat, end_beat, extrapolated_beatsteps):\n",
    "    start = extrapolated_beatsteps[start_beat]\n",
    "    end = extrapolated_beatsteps[end_beat]\n",
    "    out = copy.deepcopy(midi)\n",
    "    for note in out.instruments[0].notes.copy():\n",
    "        if note.start > end or note.start < start:\n",
    "            out.instruments[0].notes.remove(note)\n",
    "        # interpolate index of start note\n",
    "\n",
    "        # lower = len(extrapolated_beatsteps[extrapolated_beatsteps <= note.start]) - 1\n",
    "        lower = np.searchsorted(extrapolated_beatsteps, note.start, side='left') - 1\n",
    "        note.start = lower\n",
    "        note.start = int(note.start - start_beat)\n",
    "\n",
    "        lower = np.searchsorted(extrapolated_beatsteps, note.end, side='left') - 1\n",
    "        # lower = len(extrapolated_beatsteps[extrapolated_beatsteps <= note.end]) - 1\n",
    "        note.end = lower\n",
    "        note.end = int(note.end - start_beat)\n",
    "        if note.end == note.start:\n",
    "            note.end += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model, processor, and tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model = Pop2PianoForConditionalGeneration.from_pretrained(\"./cache/model\")\n",
    "processor = Pop2PianoProcessor.from_pretrained(\"./cache/processor\")\n",
    "tokenizer = Pop2PianoTokenizer.from_pretrained(\"./cache/tokenizer\")\n",
    "\n",
    "print(\"Loaded pretrained model, processor, and tokenizer.\")\n",
    "# cache the model, processor, and tokenizer to avoid downloading them again\n",
    "# model.save_pretrained(\"./cache/model\")\n",
    "# processor.save_pretrained(\"./cache/processor\")\n",
    "# tokenizer.save_pretrained(\"./cache/tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example audio file and corresponding ground truth midi file\n",
    "audio_path = \"./processed/audio/Pat Benatar - Hit Me with Your Best Shot.ogg\"\n",
    "# audio_path = \"./processed/audio/Aerosmith - Same Old Song & Dance.ogg\"\n",
    "audio, sr = librosa.load(audio_path, sr=44100)  # feel free to change the sr to a suitable value.\n",
    "\n",
    "# convert the audio file to tokens\n",
    "inputs = processor(audio=audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# load ground truth midi file\n",
    "# midi = pretty_midi.PrettyMIDI(\"./processed/midi/Mountain - Mississippi Queen.mid\")\n",
    "# ground_truth_midi_path = \"./processed/midi/Mountain - Mississippi Queen.mid\"\n",
    "# ground_truth_midi_path = \"mountain_out_gen.mid\"\n",
    "ground_truth_midi_path = \"./processed/piano_midi/Pat Benatar - Hit Me with Your Best Shot.mid\"\n",
    "midi = pretty_midi.PrettyMIDI(ground_truth_midi_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([747])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.beatsteps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the midi file to tokens\n",
    "batches = [crop_midi(midi, i, i+8, inputs.extrapolated_beatstep[0]).instruments[0].notes for i in range(2, len(inputs.extrapolated_beatstep[0])-10, 8)]\n",
    "# # remove empty batches\n",
    "# batches = [batch for batch in batches if len(batch) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Note(start=0.000000, end=1.000000, pitch=40, velocity=77),\n",
       " Note(start=0.000000, end=1.000000, pitch=52, velocity=77),\n",
       " Note(start=0.000000, end=1.000000, pitch=59, velocity=77),\n",
       " Note(start=0.000000, end=1.000000, pitch=64, velocity=77),\n",
       " Note(start=0.000000, end=1.000000, pitch=68, velocity=77),\n",
       " Note(start=0.000000, end=1.000000, pitch=71, velocity=77)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.beatsteps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "711/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "offset = 0\n",
    "for batch in batches:\n",
    "    # print(f\"outer offset: {offset}\")\n",
    "    label, offset = encode_plus(tokenizer, batch, return_tensors=\"pt\", time_offset=0)\n",
    "    labels.append(label[\"token_ids\"])\n",
    "labels = [np.append([0], np.append(label, [1, 0])) for label in labels]\n",
    "gt_longest_length = max([len(label) for label in labels])\n",
    "model_output = model.generate(inputs[\"input_features\"], generation_config=model.generation_config, return_dict_in_generate=True, output_logits=True, min_new_tokens=gt_longest_length)\n",
    "longest_length = len(model_output.sequences[0])\n",
    "padded_labels = np.array([np.pad(label, (0, longest_length - len(label))) for label in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 127)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_convert(t_labels, vocab_size):\n",
    "    # Your vocabulary size\n",
    "    vocab_size = 2400\n",
    "\n",
    "    # Create a tensor to hold the one-hot encoded versions\n",
    "    one_hot_tensor = torch.zeros((*t_labels.shape, vocab_size))\n",
    "\n",
    "    # Iterate over each element of the original tensor\n",
    "    for i in range(t_labels.size(0)):\n",
    "        for j in range(t_labels.size(1)):\n",
    "            # Get the value from the original tensor\n",
    "            value = int(t_labels[i, j])\n",
    "            # One-hot encode the value\n",
    "            one_hot = torch.zeros(vocab_size)\n",
    "            one_hot[value] = 1\n",
    "            # Assign it to the corresponding position in the new tensor\n",
    "            one_hot_tensor[i, j] = one_hot\n",
    "    return one_hot_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([126, 94, 2400])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "loss_fct = CrossEntropyLoss()\n",
    "logits = torch.stack(model_output.logits).transpose(0,1)\n",
    "logits = torch.nan_to_num(logits, nan=0.0, posinf=5, neginf=-5)\n",
    "print(logits.transpose(0,1).shape)\n",
    "t_labels = torch.tensor(padded_labels)\n",
    "t_labels = t_labels[:,1:]\n",
    "one_hot = one_hot_convert(t_labels, 2400)\n",
    "# generate one hot from t_labels\n",
    "\n",
    "# print(t_labels.shape)\n",
    "loss = loss_fct(logits, one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.2818e+00, -1.0000e+07, -1.4853e+01,  ..., -1.5166e+01,\n",
       "          -1.4933e+01, -1.5089e+01],\n",
       "         [-1.0468e+01, -1.0000e+07, -1.5300e+01,  ..., -1.5634e+01,\n",
       "          -1.5360e+01, -1.5520e+01],\n",
       "         [-8.7273e+00, -1.0000e+07, -1.5466e+01,  ..., -1.5696e+01,\n",
       "          -1.5534e+01, -1.5634e+01],\n",
       "         ...,\n",
       "         [ 4.3484e+01,  3.3260e+00, -4.7254e+00,  ..., -4.8456e+00,\n",
       "          -4.6384e+00, -4.8502e+00],\n",
       "         [ 4.3511e+01,  3.2668e+00, -4.7422e+00,  ..., -4.8624e+00,\n",
       "          -4.6527e+00, -4.8638e+00],\n",
       "         [ 4.3643e+01,  3.2227e+00, -4.7192e+00,  ..., -4.8386e+00,\n",
       "          -4.6297e+00, -4.8388e+00]],\n",
       "\n",
       "        [[-4.8721e+00, -1.0000e+07, -1.4158e+01,  ..., -1.4365e+01,\n",
       "          -1.4191e+01, -1.4358e+01],\n",
       "         [-1.2420e+01, -1.0000e+07, -1.8735e+01,  ..., -1.9031e+01,\n",
       "          -1.8883e+01, -1.9039e+01],\n",
       "         [-1.0057e+01, -1.0000e+07, -1.7442e+01,  ..., -1.7573e+01,\n",
       "          -1.7482e+01, -1.7578e+01],\n",
       "         ...,\n",
       "         [ 4.2634e+01,  5.6226e-01, -2.6722e+00,  ..., -2.8531e+00,\n",
       "          -2.6133e+00, -2.8257e+00],\n",
       "         [ 4.2796e+01,  5.3159e-01, -2.6472e+00,  ..., -2.8254e+00,\n",
       "          -2.5877e+00, -2.7985e+00],\n",
       "         [ 4.2854e+01,  4.9353e-01, -2.6985e+00,  ..., -2.8771e+00,\n",
       "          -2.6360e+00, -2.8476e+00]],\n",
       "\n",
       "        [[-3.1270e+00, -1.0000e+07, -1.4687e+01,  ..., -1.4924e+01,\n",
       "          -1.4720e+01, -1.4886e+01],\n",
       "         [-1.0478e+01, -1.0000e+07, -1.4491e+01,  ..., -1.4809e+01,\n",
       "          -1.4532e+01, -1.4708e+01],\n",
       "         [-1.0827e+01, -1.0000e+07, -1.9869e+01,  ..., -2.0097e+01,\n",
       "          -1.9957e+01, -2.0074e+01],\n",
       "         ...,\n",
       "         [ 4.6025e+01,  1.7507e+00, -7.6020e-01,  ..., -8.9834e-01,\n",
       "          -7.3540e-01, -9.4164e-01],\n",
       "         [ 4.6164e+01,  1.7481e+00, -6.3089e-01,  ..., -7.6474e-01,\n",
       "          -6.0307e-01, -8.0521e-01],\n",
       "         [ 4.6392e+01,  1.7674e+00, -5.2218e-01,  ..., -6.5573e-01,\n",
       "          -4.9125e-01, -6.9070e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.1632e+00, -1.0000e+07, -1.3754e+01,  ..., -1.4056e+01,\n",
       "          -1.3831e+01, -1.3993e+01],\n",
       "         [-9.6561e+00, -1.0000e+07, -1.9981e+01,  ..., -2.0334e+01,\n",
       "          -2.0135e+01, -2.0302e+01],\n",
       "         [-7.0906e+00, -1.0000e+07, -1.5728e+01,  ..., -1.5873e+01,\n",
       "          -1.5747e+01, -1.5826e+01],\n",
       "         ...,\n",
       "         [ 4.0362e+01,  2.9208e+00, -2.7895e+00,  ..., -2.9759e+00,\n",
       "          -2.7345e+00, -2.9496e+00],\n",
       "         [ 4.0523e+01,  2.8579e+00, -2.6095e+00,  ..., -2.7945e+00,\n",
       "          -2.5492e+00, -2.7656e+00],\n",
       "         [ 4.0678e+01,  2.8423e+00, -2.4390e+00,  ..., -2.6198e+00,\n",
       "          -2.3737e+00, -2.5881e+00]],\n",
       "\n",
       "        [[-2.7994e+00, -1.0000e+07, -1.4048e+01,  ..., -1.4338e+01,\n",
       "          -1.4129e+01, -1.4301e+01],\n",
       "         [-9.4867e+00, -1.0000e+07, -1.5240e+01,  ..., -1.5564e+01,\n",
       "          -1.5307e+01, -1.5487e+01],\n",
       "         [-8.4593e+00, -1.0000e+07, -1.5352e+01,  ..., -1.5555e+01,\n",
       "          -1.5410e+01, -1.5532e+01],\n",
       "         ...,\n",
       "         [ 4.1878e+01,  3.7016e+00, -5.7294e+00,  ..., -5.8720e+00,\n",
       "          -5.6095e+00, -5.8151e+00],\n",
       "         [ 4.1990e+01,  3.6682e+00, -5.6014e+00,  ..., -5.7427e+00,\n",
       "          -5.4807e+00, -5.6864e+00],\n",
       "         [ 4.2283e+01,  3.6880e+00, -5.3983e+00,  ..., -5.5369e+00,\n",
       "          -5.2744e+00, -5.4805e+00]],\n",
       "\n",
       "        [[-4.6972e+00, -1.0000e+07, -1.5298e+01,  ..., -1.5615e+01,\n",
       "          -1.5388e+01, -1.5566e+01],\n",
       "         [-1.0745e+01, -1.0000e+07, -1.5324e+01,  ..., -1.5651e+01,\n",
       "          -1.5391e+01, -1.5563e+01],\n",
       "         [-8.9447e+00, -1.0000e+07, -1.5386e+01,  ..., -1.5574e+01,\n",
       "          -1.5441e+01, -1.5546e+01],\n",
       "         ...,\n",
       "         [ 4.1596e+01,  2.8721e+00, -4.8777e+00,  ..., -5.0589e+00,\n",
       "          -4.8421e+00, -5.0207e+00],\n",
       "         [ 4.1698e+01,  2.8323e+00, -4.8493e+00,  ..., -5.0288e+00,\n",
       "          -4.8106e+00, -4.9892e+00],\n",
       "         [ 4.1765e+01,  2.8106e+00, -4.8533e+00,  ..., -5.0322e+00,\n",
       "          -4.8122e+00, -4.9915e+00]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3154)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.num_bars = 2\n",
    "output = tokenizer.batch_decode(np.array(padded_labels),feature_extractor_output=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['pretty_midi_objects'][0].write(\"mountain_out_sanity_check.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
