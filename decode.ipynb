{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/.pyenv/versions/3.11.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor, Pop2PianoTokenizer\n",
    "from encoder import encode_plus\n",
    "import sys\n",
    "sys.path.append(\"./pop2piano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def crop_midi(midi, start_beat, end_beat, extrapolated_beatsteps):\n",
    "    start = extrapolated_beatsteps[start_beat]\n",
    "    end = extrapolated_beatsteps[end_beat]\n",
    "    out = copy.deepcopy(midi)\n",
    "    for note in out.instruments[0].notes.copy():\n",
    "        if note.start > end or note.start < start:\n",
    "            out.instruments[0].notes.remove(note)\n",
    "        # interpolate index of start note\n",
    "\n",
    "        lower = np.argmax(extrapolated_beatsteps[extrapolated_beatsteps <= note.start])\n",
    "        note.start = lower\n",
    "        note.start = int(note.start - start_beat)\n",
    "\n",
    "        lower = np.argmax(extrapolated_beatsteps[extrapolated_beatsteps <= note.end])\n",
    "        note.end = lower\n",
    "        note.end = int(note.end - start_beat)\n",
    "        if note.end == note.start:\n",
    "            note.end += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model, processor, and tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model = Pop2PianoForConditionalGeneration.from_pretrained(\"./cache/model\")\n",
    "processor = Pop2PianoProcessor.from_pretrained(\"./cache/processor\")\n",
    "tokenizer = Pop2PianoTokenizer.from_pretrained(\"./cache/tokenizer\")\n",
    "\n",
    "print(\"Loaded pretrained model, processor, and tokenizer.\")\n",
    "# cache the model, processor, and tokenizer to avoid downloading them again\n",
    "# model.save_pretrained(\"./cache/model\")\n",
    "# processor.save_pretrained(\"./cache/processor\")\n",
    "# tokenizer.save_pretrained(\"./cache/tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example audio file and corresponding ground truth midi file\n",
    "# audio_path = \"./processed/audio/Mountain - Mississippi Queen.ogg\"\n",
    "audio_path = \"./processed/audio/Pat Benatar - Hit Me with Your Best Shot.ogg\"\n",
    "audio, sr = librosa.load(audio_path, sr=44100)  # feel free to change the sr to a suitable value.\n",
    "\n",
    "sr = int(sr)\n",
    "\n",
    "# convert the audio file to tokens\n",
    "inputs = processor(audio=audio, sampling_rate=sr, return_tensors=\"pt\", resample=True)\n",
    "\n",
    "\n",
    "# load ground truth midi file\n",
    "# midi = pretty_midi.PrettyMIDI(\"./processed/midi/Mountain - Mississippi Queen.mid\")\n",
    "# ground_truth_midi_path = \"./processed/midi/Mountain - Mississippi Queen.mid\"\n",
    "# ground_truth_midi_path = \"mountain_out_gen.mid\"\n",
    "ground_truth_midi_path = \"./processed/piano_midi/Pat Benatar - Hit Me with Your Best Shot.mid\"\n",
    "midi = pretty_midi.PrettyMIDI(ground_truth_midi_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the midi file to tokens\n",
    "batches = [crop_midi(midi, i, i+8, inputs.extrapolated_beatstep[0]).instruments[0].notes for i in range(2, len(inputs.extrapolated_beatstep[0])-8, 8)]\n",
    "# # remove empty batches\n",
    "# batches = [batch for batch in batches if len(batch) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "offset = 0\n",
    "for batch in batches:\n",
    "    print(f\"outer offset: {offset}\")\n",
    "    label, offset = encode_plus(tokenizer, batch, return_tensors=\"pt\", time_offset=0)        \n",
    "    labels.append(label[\"token_ids\"])\n",
    "labels = [np.append([0], np.append(label, [1, 0])) for label in labels]\n",
    "longest_length = max([len(label) for label in labels])\n",
    "padded_labels = np.array([np.pad(label, (0, longest_length - len(label))) for label in labels])\n",
    "print(padded_labels[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_labels[padded_labels > 135] = 135\n",
    "\n",
    "\n",
    "# # decode the tokens\n",
    "tokenizer.num_bars = 2\n",
    "output = tokenizer.batch_decode(np.array(padded_labels),feature_extractor_output=inputs)\n",
    "\n",
    "# # write the decoded midi file\n",
    "output_file_path = \"output.mid\"\n",
    "output['pretty_midi_objects'][0].write(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model.generate(inputs[\"input_features\"], generation_config=model.generation_config, return_dict_in_generate=True, output_logits=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94, 92])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.sequences.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
