{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/.pyenv/versions/3.11.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor, Pop2PianoTokenizer\n",
    "from encoder import encode_plus\n",
    "import sys\n",
    "sys.path.append(\"./pop2piano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def crop_midi(midi, start_beat, end_beat, extrapolated_beatsteps):\n",
    "    start = extrapolated_beatsteps[start_beat]\n",
    "    end = extrapolated_beatsteps[end_beat]\n",
    "    out = copy.deepcopy(midi)\n",
    "    for note in out.instruments[0].notes.copy():\n",
    "        if note.start > end or note.start < start:\n",
    "            out.instruments[0].notes.remove(note)\n",
    "        # interpolate index of start note\n",
    "\n",
    "        # lower = len(extrapolated_beatsteps[extrapolated_beatsteps <= note.start]) - 1\n",
    "        lower = np.searchsorted(extrapolated_beatsteps, note.start, side='left') - 1\n",
    "        note.start = lower\n",
    "        note.start = int(note.start - start_beat)\n",
    "\n",
    "        lower = np.searchsorted(extrapolated_beatsteps, note.end, side='left') - 1\n",
    "        # lower = len(extrapolated_beatsteps[extrapolated_beatsteps <= note.end]) - 1\n",
    "        note.end = lower\n",
    "        note.end = int(note.end - start_beat)\n",
    "        if note.end == note.start:\n",
    "            note.end += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model, processor, and tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model = Pop2PianoForConditionalGeneration.from_pretrained(\"./cache/model\")\n",
    "processor = Pop2PianoProcessor.from_pretrained(\"./cache/processor\")\n",
    "tokenizer = Pop2PianoTokenizer.from_pretrained(\"./cache/tokenizer\")\n",
    "\n",
    "print(\"Loaded pretrained model, processor, and tokenizer.\")\n",
    "# cache the model, processor, and tokenizer to avoid downloading them again\n",
    "# model.save_pretrained(\"./cache/model\")\n",
    "# processor.save_pretrained(\"./cache/processor\")\n",
    "# tokenizer.save_pretrained(\"./cache/tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pop2PianoForConditionalGeneration(\n",
       "  (shared): Embedding(2400, 512)\n",
       "  (mel_conditioner): Pop2PianoConcatEmbeddingToMel(\n",
       "    (embedding): Embedding(21, 512)\n",
       "  )\n",
       "  (encoder): Pop2PianoStack(\n",
       "    (embed_tokens): Embedding(2400, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): Pop2PianoBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): Pop2PianoLayerSelfAttention(\n",
       "            (SelfAttention): Pop2PianoAttention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): Pop2PianoLayerFF(\n",
       "            (DenseReluDense): Pop2PianoDenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x Pop2PianoBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): Pop2PianoLayerSelfAttention(\n",
       "            (SelfAttention): Pop2PianoAttention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): Pop2PianoLayerFF(\n",
       "            (DenseReluDense): Pop2PianoDenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): Pop2PianoLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Pop2PianoStack(\n",
       "    (embed_tokens): Embedding(2400, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): Pop2PianoBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): Pop2PianoLayerSelfAttention(\n",
       "            (SelfAttention): Pop2PianoAttention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): Pop2PianoLayerCrossAttention(\n",
       "            (EncDecAttention): Pop2PianoAttention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Pop2PianoLayerFF(\n",
       "            (DenseReluDense): Pop2PianoDenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x Pop2PianoBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): Pop2PianoLayerSelfAttention(\n",
       "            (SelfAttention): Pop2PianoAttention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): Pop2PianoLayerCrossAttention(\n",
       "            (EncDecAttention): Pop2PianoAttention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): Pop2PianoLayerFF(\n",
       "            (DenseReluDense): Pop2PianoDenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): Pop2PianoLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): Pop2PianoLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=2400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "decoder.final_layer_norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    if any([layer in name for layer in [\"block.5.layer.2.DenseReluDense.wo\", \"decoder.final_layer_norm\", \"lm_head\"]]):\n",
    "      print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example audio file and corresponding ground truth midi file\n",
    "audio_path = \"./processed/audio/Aerosmith - Same Old Song & Dance.ogg\"\n",
    "# audio_path = \"./processed/audio/Aerosmith - Same Old Song & Dance.ogg\"\n",
    "audio, sr = librosa.load(audio_path, sr=44100)  # feel free to change the sr to a suitable value.\n",
    "\n",
    "# convert the audio file to tokens\n",
    "inputs = processor(audio=audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# load ground truth midi file\n",
    "# midi = pretty_midi.PrettyMIDI(\"./processed/midi/Mountain - Mississippi Queen.mid\")\n",
    "# ground_truth_midi_path = \"./processed/midi/Mountain - Mississippi Queen.mid\"\n",
    "# ground_truth_midi_path = \"mountain_out_gen.mid\"\n",
    "ground_truth_midi_path = \"./processed/piano_midi/Aerosmith - Same Old Song & Dance.mid\"\n",
    "midi = pretty_midi.PrettyMIDI(ground_truth_midi_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([823])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.beatsteps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the midi file to tokens\n",
    "batches = [crop_midi(midi, i, i+8, inputs.extrapolated_beatstep[0]).instruments[0].notes for i in range(2, len(inputs.extrapolated_beatstep[0])-10, 8)]\n",
    "# # remove empty batches\n",
    "# batches = [batch for batch in batches if len(batch) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Note(start=3.000000, end=5.000000, pitch=47, velocity=77),\n",
       " Note(start=1.000000, end=5.000000, pitch=52, velocity=77),\n",
       " Note(start=5.000000, end=9.000000, pitch=47, velocity=77),\n",
       " Note(start=5.000000, end=9.000000, pitch=52, velocity=77),\n",
       " Note(start=5.000000, end=9.000000, pitch=59, velocity=77),\n",
       " Note(start=5.000000, end=9.000000, pitch=64, velocity=77),\n",
       " Note(start=5.000000, end=9.000000, pitch=68, velocity=77),\n",
       " Note(start=5.000000, end=9.000000, pitch=71, velocity=77),\n",
       " Note(start=5.000000, end=10.000000, pitch=40, velocity=77)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "823"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.beatsteps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "711/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': [134, 133, 56, 135, 132, 56, 133, 56, 63, 135, 132, 56, 63, 133, 56, 63, 56, 63, 135, 132, 56, 63, 56, 63, 133, 56, 63, 135, 132, 56, 63, 133, 56, 63, 135, 132, 56, 63, 133, 56, 63, 135, 132, 56, 63, 135, 133, 63, 135, 132, 63]}\n",
      "{'token_ids': [134, 133, 66, 56, 135, 132, 66, 133, 66, 59, 63, 135, 132, 56, 66, 133, 66, 56, 135, 132, 59, 63, 66, 133, 59, 63, 66, 135, 132, 56, 133, 56, 135, 132, 59, 63, 133, 59, 63, 135, 132, 56, 133, 56, 135, 132, 56, 59, 63, 66, 133, 44, 56, 135, 132, 44, 56]}\n",
      "{'token_ids': [134, 133, 44, 56, 135, 132, 44, 56, 133, 46, 58, 66, 135, 132, 46, 58, 66, 133, 47, 59, 66, 135, 132, 47, 59, 133, 47, 59, 135, 132, 47, 59, 133, 47, 59, 135, 132, 47, 59, 133, 47, 59, 71, 135, 132, 47, 59, 71, 135, 66, 133, 78, 137, 132, 78]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 73, 135, 78, 135, 132, 73, 78, 133, 71, 78, 44, 56, 135, 132, 71, 78, 133, 71, 78, 135, 132, 71, 78, 133, 71, 78, 135, 132, 71, 78, 133, 71, 78, 135, 132, 71, 78, 133, 78, 135, 132, 78, 137, 44, 56]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 83, 56, 135, 63, 135, 132, 78, 83, 133, 71, 78, 83, 135, 132, 63, 133, 63, 135, 132, 71, 78, 83, 133, 59, 135, 132, 59, 63, 136, 56]}\n",
      "{'token_ids': [134, 133, 59, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 44, 135, 132, 59, 133, 78, 135, 132, 78, 137, 44]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 59, 47, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 135, 132, 47, 59, 133, 49, 61, 68, 73, 135, 73, 135, 132, 49, 61, 68, 73, 73]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 135, 132, 71, 133, 71, 135, 132, 71, 133, 71, 135, 132, 71, 133, 71, 135, 132, 71, 133, 71, 135, 132, 71, 133, 71, 73, 135, 132, 71, 73, 133, 78, 136, 132, 78]}\n",
      "{'token_ids': [135, 133, 57, 64, 71, 76, 83, 138, 132, 57, 64, 71, 76, 83, 133, 71, 75, 78, 83, 136, 132, 71, 75, 78, 83, 133, 78, 135, 132, 78]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 136, 132, 78, 133, 78, 78, 135, 132, 78, 78, 133, 78, 135, 132, 78]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 80, 135, 132, 78, 80, 133, 80, 135, 132, 80, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 56, 137, 132, 78, 137, 56]}\n",
      "{'token_ids': [134, 133, 75, 136, 78, 63, 78, 135, 132, 78, 75, 133, 75, 80, 135, 132, 75, 133, 75, 135, 132, 63, 75, 78, 80, 133, 78, 71, 135, 75, 135, 132, 75, 78, 133, 72, 135, 132, 71, 72]}\n",
      "{'token_ids': [134, 133, 72, 75, 136, 132, 72, 75, 133, 72, 75, 72, 75, 135, 132, 72, 75, 72, 133, 72, 77, 135, 132, 72, 75, 133, 72, 75, 135, 132, 72, 75, 133, 72, 75, 135, 132, 72, 75, 135, 77, 133, 56, 135, 132, 56]}\n",
      "{'token_ids': [134, 133, 56, 135, 132, 56, 133, 56, 66, 135, 132, 56, 66, 133, 56, 66, 135, 132, 56, 66, 133, 56, 66, 135, 132, 56, 66, 133, 56, 66, 135, 132, 56, 66, 133, 56, 66, 135, 132, 56, 66, 133, 56, 66, 135, 132, 56, 66, 133, 72, 135, 132, 72]}\n",
      "{'token_ids': [134, 133, 75, 72, 135, 132, 75, 133, 68, 75, 56, 44, 135, 132, 68, 75, 133, 68, 75, 135, 132, 56, 68, 72, 133, 68, 72, 56, 60, 63, 77, 135, 132, 68, 72, 133, 68, 72, 80, 135, 132, 56, 133, 56, 135, 132, 68, 72, 80, 135, 44, 56, 60, 63, 75, 77, 133, 56, 44, 135, 132, 56, 139, 44]}\n",
      "{'token_ids': [134, 133, 56, 63, 135, 132, 56, 133, 66, 56, 135, 132, 66, 133, 66, 135, 132, 56, 133, 56, 68, 135, 132, 63, 66, 133, 63, 66, 135, 132, 56, 63, 66, 68, 133, 44, 56, 59, 66, 135, 132, 44, 56, 59, 66, 135, 133, 56, 78, 135, 132, 56, 137, 78]}\n",
      "{'token_ids': [134, 133, 75, 56, 136, 132, 75, 133, 75, 60, 63, 75, 135, 132, 75, 56, 133, 56, 78, 135, 132, 56, 60, 63, 75, 78, 133, 73, 47, 59, 66, 135, 132, 73, 133, 73, 135, 132, 47, 59, 73, 135, 133, 71, 59, 135, 132, 71, 135, 59, 135, 66]}\n",
      "{'token_ids': [134, 133, 71, 135, 132, 71, 133, 61, 73, 135, 132, 61, 73, 133, 75, 135, 132, 75, 133, 61, 73, 135, 132, 61, 73, 133, 52, 64, 71, 78, 136, 132, 52, 64, 71, 78, 135, 133, 76, 135, 132, 76]}\n",
      "{'token_ids': [134, 133, 76, 135, 132, 76, 133, 54, 78, 135, 61, 80, 136, 132, 54, 61, 78, 80, 133, 66, 80, 135, 132, 66, 133, 83, 56, 135, 132, 83, 133, 83, 63, 135, 132, 80, 83, 136, 56, 63]}\n",
      "{'token_ids': [134, 133, 59, 135, 132, 59, 133, 59, 54, 61, 135, 132, 59, 135, 54, 61, 133, 54, 61, 54, 61, 66, 69, 135, 132, 54, 61, 54, 133, 54, 63, 135, 132, 54, 61, 63, 66, 69, 133, 51, 61, 63, 135, 132, 51, 61, 63, 135, 133, 59, 135, 132, 59]}\n",
      "{'token_ids': [134, 133, 56, 60, 63, 136, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 66, 135, 132, 66, 133, 59, 47, 61, 135, 132, 59, 133, 59, 135, 132, 47, 59, 61, 135, 133, 59, 71, 135, 132, 59, 71]}\n",
      "{'token_ids': [134, 133, 59, 71, 136, 132, 59, 71, 133, 61, 73, 135, 132, 61, 73, 133, 61, 73, 61, 73, 135, 132, 61, 73, 61, 73, 133, 52, 59, 71, 78, 135, 132, 52, 59, 71, 78, 133, 52, 59, 71, 78, 135, 132, 52, 59, 71, 78, 135, 133, 59, 135, 132, 59]}\n",
      "{'token_ids': [134, 133, 52, 59, 64, 135, 132, 52, 59, 64, 133, 42, 54, 66, 135, 132, 42, 54, 133, 42, 54, 68, 136, 132, 42, 54, 66, 135, 133, 44, 56, 71, 83, 136, 132, 44, 56, 71, 83, 133, 83, 59, 136, 132, 68, 83, 59]}\n",
      "{'token_ids': [135, 133, 83, 54, 42, 83, 135, 132, 83, 54, 133, 54, 78, 136, 132, 54, 78, 133, 61, 54, 78, 80, 135, 132, 61, 133, 61, 135, 132, 54, 135, 42, 61, 78, 80, 83, 133, 59, 135, 132, 59]}\n",
      "{'token_ids': [134, 133, 59, 135, 132, 59, 133, 44, 56, 136, 132, 44, 56, 133, 44, 56, 135, 132, 44, 56, 133, 44, 56, 135, 132, 44, 56, 133, 44, 56, 135, 132, 44, 56, 135, 133, 63, 56, 135, 132, 63, 135, 56]}\n",
      "{'token_ids': [134, 133, 63, 135, 66, 56, 135, 132, 66, 133, 66, 135, 132, 63, 133, 63, 135, 132, 56, 133, 56, 135, 132, 63, 66, 133, 63, 66, 71, 135, 132, 56, 133, 56, 135, 132, 56, 63, 66, 71, 133, 56, 60, 63, 135, 132, 56, 60, 63]}\n",
      "{'token_ids': [134, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 56, 60, 63, 136, 132, 56, 60, 63, 133, 56, 60, 63, 56, 60, 63, 135, 132, 56, 60, 63, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 135, 133, 56, 60, 63, 135, 132, 56, 135, 60, 63]}\n",
      "{'token_ids': [134, 133, 56, 135, 66, 60, 63, 135, 132, 66, 133, 66, 136, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 66, 133, 66, 135, 132, 56, 60, 63, 66, 133, 63, 66, 135, 132, 63, 66, 133, 71, 135, 132, 71]}\n",
      "{'token_ids': [134, 133, 44, 56, 72, 75, 135, 132, 44, 56, 72, 75, 133, 44, 56, 72, 75, 135, 132, 44, 56, 72, 75, 133, 44, 56, 72, 75, 135, 132, 44, 56, 72, 75, 133, 44, 56, 72, 75, 78, 135, 132, 44, 56, 72, 75, 78, 133, 44, 56, 72, 75, 78, 135, 132, 44, 56, 133, 44, 56, 135, 132, 44, 56, 72, 75, 78, 135, 133, 56, 72, 75, 44, 135, 132, 56, 135, 72, 75, 139, 44]}\n",
      "{'token_ids': [134, 133, 56, 135, 132, 56, 133, 56, 78, 72, 75, 135, 132, 56, 78, 133, 78, 56, 135, 132, 78, 133, 78, 63, 135, 132, 78, 133, 78, 135, 132, 56, 78, 133, 56, 78, 135, 132, 56, 63, 72, 75, 78, 135, 133, 56, 60, 63, 135, 132, 56, 60, 63]}\n",
      "{'token_ids': [134, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 133, 56, 60, 63, 135, 132, 56, 60, 63, 135, 133, 56, 44, 135, 132, 56, 140, 44]}\n",
      "{'token_ids': [134, 133, 56, 135, 132, 56, 133, 56, 66, 135, 132, 56, 66, 133, 56, 66, 135, 132, 56, 66, 133, 56, 66, 135, 132, 56, 66, 133, 56, 63, 66, 135, 132, 56, 63, 66, 133, 56, 63, 66, 135, 132, 56, 63, 66, 135, 133, 56, 71, 137, 132, 56, 135, 71]}\n",
      "{'token_ids': [134, 133, 75, 60, 63, 136, 132, 75, 133, 56, 75, 135, 132, 56, 60, 63, 75, 133, 58, 135, 132, 58, 133, 47, 59, 66, 71, 75, 136, 132, 47, 59, 66, 71, 75, 135, 133, 71, 135, 132, 71]}\n",
      "{'token_ids': [134, 133, 71, 136, 132, 71, 133, 73, 61, 68, 73, 135, 132, 73, 135, 61, 68, 73, 133, 73, 71, 76, 64, 68, 135, 132, 73, 135, 71, 133, 71, 135, 132, 71, 76, 133, 76, 135, 132, 76, 135, 64, 68]}\n",
      "{'token_ids': [134, 133, 76, 135, 132, 76, 133, 54, 61, 70, 78, 135, 80, 135, 132, 80, 133, 80, 135, 132, 54, 61, 70, 78, 80, 133, 83, 56, 63, 66, 75, 136, 132, 83, 135, 133, 59, 135, 132, 56, 59, 63, 66, 75]}\n",
      "{'token_ids': [134, 133, 54, 135, 132, 54, 133, 54, 61, 66, 135, 68, 80, 135, 132, 68, 80, 133, 68, 80, 135, 132, 54, 61, 66, 68, 80, 133, 68, 80, 135, 132, 68, 80, 133, 71, 83, 135, 132, 71, 83, 135, 133, 59, 135, 132, 59]}\n",
      "{'token_ids': [134, 133, 56, 44, 72, 75, 80, 137, 132, 56, 133, 56, 135, 132, 44, 56, 72, 75, 80, 133, 47, 59, 66, 71, 75, 136, 132, 47, 59, 135, 66, 71, 75, 133, 59, 135, 132, 59]}\n",
      "{'token_ids': [134, 133, 59, 135, 132, 59, 133, 61, 49, 135, 132, 61, 133, 61, 135, 132, 49, 61, 133, 61, 135, 132, 61, 133, 52, 59, 64, 66, 135, 132, 52, 59, 64, 66, 133, 52, 59, 64, 66, 135, 132, 52, 59, 64, 66, 133, 52, 59, 64, 66, 135, 132, 52, 59, 64, 66, 133, 76, 137, 132, 76]}\n",
      "{'token_ids': [136, 133, 54, 61, 66, 69, 78, 135, 132, 54, 61, 66, 69, 78, 136, 133, 45, 57, 64, 69, 71, 135, 132, 45, 57, 64, 69, 135, 71, 133, 57, 69, 135, 132, 57, 69]}\n",
      "{'token_ids': [134, 133, 57, 69, 135, 132, 57, 69, 133, 71, 59, 47, 135, 132, 71, 133, 71, 54, 136, 132, 59, 71, 47, 54, 133, 59, 71, 49, 61, 68, 71, 135, 132, 59, 71, 135, 133, 78, 135, 132, 78, 135, 49, 61, 68, 71]}\n",
      "{'token_ids': [134, 133, 78, 90, 49, 135, 132, 78, 90, 133, 78, 90, 56, 63, 78, 90, 135, 132, 78, 90, 56, 133, 56, 77, 80, 89, 135, 132, 49, 133, 49, 61, 135, 132, 56, 133, 56, 85, 73, 135, 132, 49, 133, 49, 135, 132, 49, 56, 61, 63, 77, 78, 80, 85, 89, 90, 135, 73, 133, 73, 80, 135, 132, 73, 80]}\n",
      "{'token_ids': [134, 133, 73, 80, 135, 132, 73, 80, 133, 73, 80, 135, 132, 73, 80, 133, 71, 75, 80, 135, 132, 71, 75, 80, 133, 47, 59, 71, 75, 80, 135, 132, 47, 59, 71, 75, 80, 135, 133, 44, 56, 71, 75, 80, 135, 132, 44, 56, 71, 75, 80, 135, 133, 59, 135, 132, 59]}\n",
      "{'token_ids': [134, 133, 63, 135, 132, 63, 133, 54, 42, 71, 136, 132, 54, 133, 54, 135, 132, 42, 71, 133, 61, 66, 71, 135, 75, 135, 132, 54, 61, 66, 71, 75, 133, 51, 80, 75, 135, 78, 135, 132, 51, 78, 80, 135, 75]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 44, 83, 75, 135, 51, 135, 80, 135, 132, 51, 133, 51, 78, 135, 132, 44, 83, 133, 83, 44, 135, 132, 51, 83, 133, 51, 83, 135, 132, 51, 83, 133, 75, 87, 135, 132, 44, 75, 75, 78, 80, 87]}\n",
      "{'token_ids': [134, 133, 75, 87, 135, 132, 75, 87, 133, 54, 42, 78, 90, 135, 80, 92, 135, 132, 54, 133, 54, 75, 87, 135, 132, 42, 54, 133, 42, 54, 72, 84, 135, 132, 42, 54, 72, 75, 78, 80, 84, 87, 90, 92, 133, 39, 51, 72, 84, 135, 132, 39, 51, 135, 72, 84, 133, 72, 84, 135, 132, 72, 146, 84]}\n",
      "{'token_ids': [134, 133, 56, 72, 75, 80, 44, 137, 132, 56, 133, 56, 135, 132, 72, 75, 80, 133, 60, 63, 72, 75, 80, 136, 132, 56, 60, 63, 72, 75, 80, 135, 133, 78, 135, 132, 44, 78]}\n",
      "{'token_ids': [134, 133, 75, 135, 132, 75, 133, 71, 42, 54, 135, 132, 71, 133, 71, 136, 132, 42, 54, 71, 133, 71, 42, 54, 63, 71, 135, 132, 71, 42, 54, 63, 133, 42, 54, 63, 77, 135, 132, 42, 54, 63, 71, 133, 56, 44, 135, 132, 56, 133, 63, 135, 132, 44, 63, 77]}\n",
      "{'token_ids': [134, 133, 63, 73, 85, 135, 132, 63, 73, 85, 133, 72, 84, 56, 44, 135, 132, 72, 84, 133, 72, 84, 135, 132, 56, 133, 85, 56, 73, 135, 132, 85, 133, 63, 60, 75, 85, 87, 135, 132, 63, 133, 63, 135, 132, 56, 135, 44, 60, 63, 72, 73, 75, 84, 85, 87, 133, 72, 84, 136, 132, 72, 84]}\n",
      "{'token_ids': [135, 133, 72, 84, 54, 42, 75, 87, 135, 132, 72, 84, 135, 54, 133, 54, 80, 92, 135, 132, 42, 54, 133, 42, 54, 135, 132, 42, 54, 75, 80, 87, 92, 133, 78, 90, 135, 132, 78, 90, 135, 133, 59, 90, 135, 132, 59, 143, 90]}\n",
      "{'token_ids': [134, 133, 59, 83, 135, 85, 135, 132, 59, 133, 87, 59, 135, 132, 87, 133, 87, 135, 132, 87, 133, 87, 135, 132, 87, 133, 87, 135, 132, 87, 133, 87, 135, 132, 87, 133, 63, 135, 132, 63, 85, 135, 59, 83]}\n",
      "{'token_ids': [134, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 61, 68, 135, 132, 61, 68]}\n",
      "{'token_ids': [134, 133, 61, 68, 135, 132, 61, 68, 133, 47, 59, 71, 135, 132, 47, 59, 71, 133, 61, 135, 132, 61, 133, 61, 135, 132, 61, 133, 51, 58, 63, 135, 132, 51, 58, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 73, 135, 132, 73]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 135, 132, 71, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 75, 63, 70, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 73, 135, 132, 73, 137, 63, 70]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 135, 132, 71, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 75, 63, 70, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 73, 135, 132, 73, 137, 63, 70]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 135, 132, 71, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 75, 63, 70, 135, 132, 75, 133, 75, 135, 132, 75, 133, 73, 135, 132, 63, 70, 73]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 59, 135, 132, 71, 133, 73, 135, 132, 73, 133, 73, 135, 132, 59, 73, 133, 75, 63, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 73, 61, 135, 132, 73, 135, 61, 63]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 59, 135, 132, 71, 133, 71, 135, 132, 59, 71, 133, 61, 73, 136, 132, 61, 73, 133, 63, 75, 51, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 132, 75, 133, 66, 78, 135, 132, 63, 135, 51, 66, 78]}\n",
      "{'token_ids': [135, 133, 68, 80, 54, 61, 135, 132, 68, 80, 133, 68, 80, 68, 80, 135, 132, 68, 80, 135, 54, 61, 68, 80, 133, 71, 83, 56, 75, 87, 44, 135, 132, 71, 83, 135, 56, 75, 87, 135, 44, 133, 73, 135, 132, 73]}\n",
      "{'token_ids': [134, 133, 73, 135, 54, 135, 61, 135, 132, 73, 133, 66, 83, 135, 70, 73, 80, 136, 132, 66, 133, 66, 135, 132, 83, 133, 63, 135, 132, 54, 61, 63, 66, 70, 73, 80]}\n",
      "{'token_ids': [134, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 73, 135, 132, 73]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 135, 132, 71, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 63, 70, 73, 135, 132, 63, 70, 73, 133, 63, 70, 73, 135, 132, 63, 70, 73, 133, 63, 70, 73, 135, 132, 63, 70, 73, 133, 73, 135, 132, 73]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 59, 135, 132, 71, 133, 71, 135, 132, 59, 71, 133, 73, 135, 132, 73, 133, 75, 63, 70, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 73, 61, 135, 132, 63, 70, 73, 135, 61]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 59, 135, 132, 71, 133, 73, 135, 132, 59, 73, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 73, 63, 70, 135, 132, 73, 135, 63, 70, 133, 73, 135, 132, 73]}\n",
      "{'token_ids': [134, 133, 73, 135, 132, 73, 133, 71, 135, 132, 71, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 75, 63, 70, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 63, 68, 135, 132, 63, 63, 68, 137, 70]}\n",
      "{'token_ids': [134, 133, 63, 68, 135, 132, 63, 68, 133, 63, 68, 71, 135, 132, 63, 68, 71, 133, 63, 68, 71, 135, 132, 63, 68, 71, 133, 63, 68, 71, 135, 132, 63, 68, 71, 133, 63, 68, 71, 135, 132, 63, 68, 71, 133, 63, 68, 71, 135, 132, 63, 68, 71, 133, 63, 68, 71, 135, 132, 63, 68, 71, 133, 75, 135, 132, 75]}\n",
      "{'token_ids': [134, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75, 133, 75, 135, 132, 75]}\n",
      "{'token_ids': [134, 133, 75, 135, 132, 75, 133, 71, 136, 132, 71, 133, 73, 135, 132, 73, 133, 73, 135, 132, 73, 133, 75, 51, 135, 58, 135, 63, 135, 132, 75, 135, 58, 63, 137, 51]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 61, 66, 73, 80, 136, 132, 61, 66, 73, 80, 133, 61, 66, 73, 80, 135, 132, 61, 66, 73, 80, 133, 71, 83, 135, 132, 71, 83, 133, 56, 75, 87, 44, 136, 132, 56, 75, 87, 139, 44]}\n",
      "{'token_ids': [134, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 61, 49, 56, 136, 132, 61, 49, 56, 133, 61, 49, 56, 61, 135, 132, 61, 49, 56, 61, 133, 51, 58, 61, 135, 132, 51, 58, 61, 135, 133, 63, 135, 132, 63]}\n",
      "{'token_ids': [134, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 135, 132, 63, 133, 63, 136, 132, 63, 133, 63, 73, 135, 132, 63, 135, 73]}\n",
      "{'token_ids': [135, 133, 73, 54, 42, 73, 78, 135, 132, 73, 135, 54, 133, 54, 80, 61, 66, 135, 132, 54, 80, 133, 54, 80, 135, 132, 42, 54, 61, 66, 73, 78, 80, 133, 51, 75, 79, 39, 136, 132, 51, 75, 79, 133, 59, 135, 132, 39, 59]}\n",
      "{'token_ids': [134, 133, 44, 135, 56, 135, 132, 56, 133, 56, 135, 132, 56, 133, 56, 135, 132, 56, 133, 56, 135, 132, 56, 133, 56, 135, 132, 56, 133, 79, 56, 135, 132, 79, 133, 56, 60, 63, 135, 132, 44, 56, 56, 135, 60, 63]}\n",
      "{'token_ids': [134, 133, 56, 73, 135, 132, 56, 73, 133, 78, 56, 44, 135, 132, 78, 133, 78, 135, 132, 56, 78, 133, 56, 63, 78, 135, 132, 56, 133, 56, 80, 135, 132, 63, 78, 133, 78, 63, 135, 132, 78, 133, 78, 135, 132, 56, 133, 44, 56, 60, 63, 135, 132, 44, 44, 56, 60, 63, 63, 135, 78, 80]}\n",
      "{'token_ids': [134, 133, 44, 56, 60, 63, 135, 132, 44, 56, 60, 63, 133, 44, 56, 60, 63, 135, 132, 44, 56, 60, 63, 133, 44, 56, 60, 63, 136, 132, 44, 56, 60, 63, 133, 44, 56, 60, 63, 44, 56, 60, 63, 135, 132, 44, 56, 60, 63, 44, 56, 60, 63, 133, 44, 56, 60, 63, 135, 132, 44, 56, 60, 63, 135, 133, 63, 56, 135, 132, 63, 135, 56]}\n",
      "{'token_ids': [134, 133, 63, 135, 132, 63, 133, 56, 63, 66, 135, 132, 56, 63, 66, 133, 56, 63, 66, 135, 132, 56, 63, 133, 56, 63, 135, 132, 56, 63, 66, 133, 56, 63, 66, 135, 132, 56, 63, 66, 133, 56, 63, 66, 135, 132, 56, 63, 66, 135, 133, 56, 73, 135, 132, 56, 138, 73]}\n",
      "{'token_ids': [134, 133, 56, 68, 75, 135, 132, 56, 133, 56, 135, 132, 68, 133, 68, 135, 132, 56, 133, 56, 135, 132, 56, 68, 133, 44, 68, 135, 51, 135, 132, 44, 133, 44, 135, 132, 51, 133, 44, 56, 72, 75, 135, 132, 44, 44, 56, 72, 75, 75, 137, 68]}\n",
      "{'token_ids': [134, 133, 56, 72, 75, 44, 135, 132, 56, 72, 75, 133, 72, 78, 63, 56, 135, 132, 72, 78, 133, 72, 78, 135, 132, 63, 72, 78, 133, 78, 63, 72, 135, 132, 78, 133, 78, 135, 132, 44, 56, 63, 72, 78, 133, 71, 78, 135, 132, 71, 78, 135, 133, 56, 135, 132, 56]}\n",
      "{'token_ids': [134, 133, 56, 60, 63, 44, 136, 132, 56, 133, 56, 135, 132, 56, 60, 63, 133, 56, 60, 63, 66, 135, 132, 56, 60, 63, 133, 56, 60, 63, 68, 135, 132, 44, 133, 44, 135, 132, 44, 56, 60, 63, 135, 66, 68, 133, 56, 72, 44, 135, 132, 56, 135, 72, 139, 44]}\n",
      "{'token_ids': [134, 133, 56, 135, 72, 78, 60, 63, 135, 132, 72, 78, 133, 72, 78, 135, 132, 56, 72, 78, 133, 72, 78, 56, 135, 132, 60, 63, 72, 78, 133, 72, 60, 63, 78, 135, 132, 72, 133, 72, 80, 135, 132, 56, 60, 63, 72, 78, 80]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 78, 135, 132, 78, 133, 47, 59, 66, 71, 73, 135, 132, 47, 59, 66, 71, 73, 133, 59, 135, 132, 59]}\n",
      "{'token_ids': [134, 133, 59, 136, 132, 59, 133, 61, 49, 61, 135, 132, 61, 49, 61, 133, 61, 135, 132, 61, 133, 52, 59, 64, 136, 132, 52, 59, 64, 133, 52, 59, 64, 135, 132, 52, 59, 64, 133, 52, 64, 71, 135, 132, 52, 64, 71]}\n",
      "{'token_ids': [134, 133, 52, 135, 132, 52, 133, 54, 73, 78, 42, 135, 132, 54, 73, 78, 133, 54, 73, 78, 136, 132, 42, 54, 73, 78, 133, 80, 44, 71, 80, 135, 132, 80, 135, 44, 133, 44, 135, 132, 44, 71, 80, 133, 71, 80, 136, 132, 71, 80]}\n",
      "{'token_ids': [135, 133, 71, 80, 71, 80, 42, 135, 132, 71, 80, 71, 133, 71, 78, 49, 135, 132, 71, 78, 133, 71, 56, 78, 135, 132, 71, 80, 133, 71, 75, 80, 135, 132, 42, 49, 56, 71, 75, 78, 80, 133, 71, 75, 135, 132, 71, 75, 135, 133, 59, 63, 135, 132, 59, 63]}\n",
      "{'token_ids': [134, 133, 56, 44, 80, 92, 137, 132, 56, 133, 56, 135, 132, 44, 56, 80, 92, 133, 59, 47, 136, 132, 59, 135, 133, 59, 66, 136, 132, 47, 59, 66]}\n",
      "{'token_ids': [135, 133, 59, 66, 61, 49, 135, 132, 59, 66, 135, 61, 133, 61, 61, 135, 132, 61, 49, 61, 133, 52, 59, 64, 68, 137, 76, 135, 132, 52, 59, 64, 68, 76]}\n",
      "{'token_ids': [134, 133, 76, 135, 132, 76, 133, 78, 54, 61, 66, 135, 132, 78, 133, 80, 135, 132, 54, 61, 66, 80, 135, 133, 57, 64, 69, 73, 76, 83, 135, 132, 57, 64, 69, 73, 76, 83, 136, 133, 57, 135, 132, 57]}\n",
      "{'token_ids': [134, 133, 57, 135, 132, 57, 133, 59, 47, 135, 132, 59, 133, 59, 135, 132, 59, 133, 59, 135, 132, 47, 59, 133, 49, 61, 88, 80, 135, 132, 49, 133, 49, 135, 132, 49, 61, 88, 135, 133, 49, 85, 135, 132, 80, 136, 49, 85]}\n",
      "{'token_ids': [135, 133, 56, 83, 135, 49, 85, 135, 132, 56, 83, 133, 56, 83, 135, 132, 49, 85, 133, 49, 61, 85, 135, 132, 56, 133, 56, 90, 135, 132, 49, 56, 61, 83, 85, 90, 135, 133, 80, 136, 132, 80]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 80, 52, 59, 64, 135, 132, 80, 133, 78, 80, 135, 132, 52, 59, 64, 78, 80, 133, 83, 47, 59, 135, 132, 83, 133, 83, 135, 132, 47, 59, 83, 133, 80, 87, 44, 135, 132, 80, 87, 135, 133, 56, 135, 132, 56, 135, 44]}\n",
      "{'token_ids': [134, 133, 56, 135, 132, 56, 133, 42, 54, 75, 87, 135, 132, 42, 54, 133, 54, 42, 135, 132, 54, 133, 54, 58, 61, 135, 132, 54, 133, 54, 135, 132, 42, 54, 58, 61, 133, 54, 71, 80, 135, 132, 54, 75, 133, 54, 59, 63, 135, 78, 135, 132, 54, 59, 63, 71, 78, 80, 87]}\n",
      "{'token_ids': [134, 133, 78, 135, 132, 78, 133, 80, 44, 135, 78, 135, 132, 78, 133, 78, 135, 132, 80, 133, 75, 80, 87, 135, 132, 75, 80, 87, 133, 56, 75, 80, 87, 135, 132, 56, 133, 56, 135, 132, 75, 80, 87, 133, 63, 78, 90, 135, 132, 44, 56, 63, 78, 78, 90]}\n",
      "{'token_ids': [134, 133, 56, 78, 87, 135, 132, 56, 78, 87, 133, 78, 42, 87, 135, 54, 135, 58, 61, 135, 132, 78, 133, 78, 84, 135, 132, 42, 54, 58, 61, 78, 84, 87, 133, 78, 85, 135, 132, 78, 85, 135, 133, 56, 135, 132, 56]}\n",
      "{'token_ids': [134, 133, 44, 68, 72, 75, 80, 135, 56, 135, 132, 44, 133, 44, 135, 132, 56, 68, 72, 75, 133, 56, 68, 72, 75, 135, 132, 44, 133, 44, 135, 132, 56, 133, 56, 135, 132, 44, 56, 68, 72, 75, 133, 44, 56, 68, 72, 75, 135, 132, 44, 56, 68, 72, 75, 133, 47, 135, 132, 80, 135, 47]}\n",
      "{'token_ids': [134, 133, 56, 135, 132, 56, 133, 42, 78, 87, 135, 54, 135, 132, 42, 133, 42, 135, 132, 54, 133, 54, 136, 132, 42, 54, 78, 87, 133, 78, 83, 42, 54, 135, 132, 78, 83, 133, 47, 135, 132, 42, 47, 54]}\n",
      "{'token_ids': [134, 133, 47, 135, 132, 47, 133, 44, 32, 71, 75, 80, 137, 132, 44, 32, 133, 44, 132, 71, 75, 80, 133, 44, 75, 80, 83, 87, 32, 135, 132, 44, 135, 44, 136, 75, 80, 83, 87, 136, 32]}\n",
      "{'token_ids': [134, 133, 59, 59, 135, 132, 59, 59, 133, 42, 54, 135, 132, 42, 54, 133, 42, 54, 135, 132, 42, 54, 133, 42, 54, 135, 132, 42, 54, 133, 42, 54, 73, 82, 135, 132, 42, 54, 73, 82, 133, 47, 59, 71, 75, 78, 136, 132, 47, 59, 71, 75, 78, 133, 47, 135, 132, 47]}\n",
      "{'token_ids': [134, 133, 44, 32, 72, 75, 80, 138, 132, 44, 133, 44, 135, 132, 44, 133, 44, 135, 132, 44, 135, 32, 72, 75, 80, 133, 44, 32, 68, 138, 132, 44, 136, 32, 138, 68]}\n",
      "{'token_ids': [137, 133, 44, 136, 32, 136, 132, 44, 133, 51, 136, 132, 32, 136, 51]}\n",
      "{'token_ids': [135, 133, 56, 136, 51, 136, 132, 51, 56, 133, 51, 56, 63, 68, 72, 75, 44, 138, 132, 51, 56, 63, 68, 72, 75, 135, 44]}\n",
      "{'token_ids': []}\n",
      "{'token_ids': []}\n",
      "{'token_ids': []}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mappend(label, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m      9\u001b[0m gt_longest_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels])\n\u001b[0;32m---> 10\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgt_longest_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m longest_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_output\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m padded_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mpad(label, (\u001b[38;5;241m0\u001b[39m, longest_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(label))) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/models/pop2piano/modeling_pop2piano.py:1297\u001b[0m, in \u001b[0;36mPop2PianoForConditionalGeneration.generate\u001b[0;34m(self, input_features, attention_mask, composer, generation_config, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# to control the variation of generated MIDI tokens we concatenate mel-conditioner tokens(which depends on composer_token)\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# at the front of input_features.\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m input_features, attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mel_conditioner_outputs(\n\u001b[1;32m   1291\u001b[0m     input_features\u001b[38;5;241m=\u001b[39minput_features,\n\u001b[1;32m   1292\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1293\u001b[0m     composer\u001b[38;5;241m=\u001b[39mcomposer,\n\u001b[1;32m   1294\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   1295\u001b[0m )\n\u001b[0;32m-> 1297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m tracing_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state()\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_state \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mScriptMethod):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1512\u001b[0m recording_scopes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         tracing_state\u001b[38;5;241m.\u001b[39mpush_scope(name)\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/models/pop2piano/modeling_pop2piano.py:1170\u001b[0m, in \u001b[0;36mPop2PianoForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, input_features, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shift_right(labels)\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;66;03m# Rescale output before projecting on vocab\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;66;03m# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m tracing_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state()\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_state \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mScriptMethod):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1512\u001b[0m recording_scopes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         tracing_state\u001b[38;5;241m.\u001b[39mpush_scope(name)\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/models/pop2piano/modeling_pop2piano.py:913\u001b[0m, in \u001b[0;36mPop2PianoStack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    899\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m    900\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         output_attentions,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m tracing_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state()\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_state \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mScriptMethod):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1512\u001b[0m recording_scopes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         tracing_state\u001b[38;5;241m.\u001b[39mpush_scope(name)\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/models/pop2piano/modeling_pop2piano.py:599\u001b[0m, in \u001b[0;36mPop2PianoBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    597\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 599\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    609\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m tracing_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state()\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_state \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mScriptMethod):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1512\u001b[0m recording_scopes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         tracing_state\u001b[38;5;241m.\u001b[39mpush_scope(name)\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/models/pop2piano/modeling_pop2piano.py:504\u001b[0m, in \u001b[0;36mPop2PianoLayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    495\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    502\u001b[0m ):\n\u001b[1;32m    503\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    514\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m tracing_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state()\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_state \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mScriptMethod):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1512\u001b[0m recording_scopes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;66;03m# type ignore was added because at this point one knows that\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         tracing_state\u001b[38;5;241m.\u001b[39mpush_scope(name)\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/models/pop2piano/modeling_pop2piano.py:428\u001b[0m, in \u001b[0;36mPop2PianoAttention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    425\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    426\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 428\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[1;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[1;32m    434\u001b[0m     query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    435\u001b[0m )  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/transformers/models/pop2piano/modeling_pop2piano.py:409\u001b[0m, in \u001b[0;36mPop2PianoAttention.forward.<locals>.project\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, key_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m past_key_value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m key_value_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;66;03m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    415\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "offset = 0\n",
    "for batch in batches:\n",
    "    # print(f\"outer offset: {offset}\")\n",
    "    label, offset = encode_plus(tokenizer, batch, return_tensors=\"pt\", time_offset=0)\n",
    "    print(label)\n",
    "    labels.append(label[\"token_ids\"])\n",
    "labels = [np.append([0], np.append(label, [1, 0])) for label in labels]\n",
    "gt_longest_length = max([len(label) for label in labels])\n",
    "model_output = model.generate(inputs[\"input_features\"], generation_config=model.generation_config, return_dict_in_generate=True, output_logits=True, min_new_tokens=gt_longest_length)\n",
    "longest_length = len(model_output.sequences[0])\n",
    "padded_labels = np.array([np.pad(label, (0, longest_length - len(label))) for label in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  0, 134, 133,  56, 135, 132,  56, 133,  56,  63, 135, 132,  56,\n",
       "         63, 133,  56,  63,  56,  63, 135, 132,  56,  63,  56,  63, 133,\n",
       "         56,  63, 135, 132,  56,  63, 133,  56,  63, 135, 132,  56,  63,\n",
       "        133,  56,  63, 135, 132,  56,  63, 135, 133,  63, 135, 132,  63,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  66,  56, 135, 132,  66, 133,  66,  59,  63, 135,\n",
       "        132,  56,  66, 133,  66,  56, 135, 132,  59,  63,  66, 133,  59,\n",
       "         63,  66, 135, 132,  56, 133,  56, 135, 132,  59,  63, 133,  59,\n",
       "         63, 135, 132,  56, 133,  56, 135, 132,  56,  59,  63,  66, 133,\n",
       "         44,  56, 135, 132,  44,  56,   1,   0]),\n",
       " array([  0, 134, 133,  44,  56, 135, 132,  44,  56, 133,  46,  58,  66,\n",
       "        135, 132,  46,  58,  66, 133,  47,  59,  66, 135, 132,  47,  59,\n",
       "        133,  47,  59, 135, 132,  47,  59, 133,  47,  59, 135, 132,  47,\n",
       "         59, 133,  47,  59,  71, 135, 132,  47,  59,  71, 135,  66, 133,\n",
       "         78, 137, 132,  78,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  73, 135,  78, 135, 132,\n",
       "         73,  78, 133,  71,  78,  44,  56, 135, 132,  71,  78, 133,  71,\n",
       "         78, 135, 132,  71,  78, 133,  71,  78, 135, 132,  71,  78, 133,\n",
       "         71,  78, 135, 132,  71,  78, 133,  78, 135, 132,  78, 137,  44,\n",
       "         56,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  78, 135, 132,  78, 133,\n",
       "         78, 135, 132,  78, 133,  78,  83,  56, 135,  63, 135, 132,  78,\n",
       "         83, 133,  71,  78,  83, 135, 132,  63, 133,  63, 135, 132,  71,\n",
       "         78,  83, 133,  59, 135, 132,  59,  63, 136,  56,   1,   0]),\n",
       " array([  0, 134, 133,  59, 135, 132,  59, 133,  59, 135, 132,  59, 133,\n",
       "         59, 135, 132,  59, 133,  59, 135, 132,  59, 133,  59, 135, 132,\n",
       "         59, 133,  59, 135, 132,  59, 133,  59,  44, 135, 132,  59, 133,\n",
       "         78, 135, 132,  78, 137,  44,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  78, 135, 132,  78, 133,\n",
       "         59,  47, 135, 132,  59, 133,  59, 135, 132,  59, 133,  59, 135,\n",
       "        132,  59, 133,  59, 135, 132,  47,  59, 133,  49,  61,  68,  73,\n",
       "        135,  73, 135, 132,  49,  61,  68,  73,  73,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71, 135, 132,  71, 133,\n",
       "         71, 135, 132,  71, 133,  71, 135, 132,  71, 133,  71, 135, 132,\n",
       "         71, 133,  71, 135, 132,  71, 133,  71,  73, 135, 132,  71,  73,\n",
       "        133,  78, 136, 132,  78,   1,   0]),\n",
       " array([  0, 135, 133,  57,  64,  71,  76,  83, 138, 132,  57,  64,  71,\n",
       "         76,  83, 133,  71,  75,  78,  83, 136, 132,  71,  75,  78,  83,\n",
       "        133,  78, 135, 132,  78,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  78, 135, 132,  78, 133,\n",
       "         78, 135, 132,  78, 133,  78, 135, 132,  78, 133,  78, 136, 132,\n",
       "         78, 133,  78,  78, 135, 132,  78,  78, 133,  78, 135, 132,  78,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  78, 135, 132,  78, 133,\n",
       "         78, 135,  80, 135, 132,  78,  80, 133,  80, 135, 132,  80, 133,\n",
       "         78, 135, 132,  78, 133,  78, 135, 132,  78, 133,  78, 135, 132,\n",
       "         78,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  78, 135, 132,  78, 133,\n",
       "         78, 135, 132,  78, 133,  78, 135, 132,  78, 133,  78, 135, 132,\n",
       "         78, 133,  78, 135, 132,  78, 133,  78, 135, 132,  78, 133,  78,\n",
       "         56, 137, 132,  78, 137,  56,   1,   0]),\n",
       " array([  0, 134, 133,  75, 136,  78,  63,  78, 135, 132,  78,  75, 133,\n",
       "         75,  80, 135, 132,  75, 133,  75, 135, 132,  63,  75,  78,  80,\n",
       "        133,  78,  71, 135,  75, 135, 132,  75,  78, 133,  72, 135, 132,\n",
       "         71,  72,   1,   0]),\n",
       " array([  0, 134, 133,  72,  75, 136, 132,  72,  75, 133,  72,  75,  72,\n",
       "         75, 135, 132,  72,  75,  72, 133,  72,  77, 135, 132,  72,  75,\n",
       "        133,  72,  75, 135, 132,  72,  75, 133,  72,  75, 135, 132,  72,\n",
       "         75, 135,  77, 133,  56, 135, 132,  56,   1,   0]),\n",
       " array([  0, 134, 133,  56, 135, 132,  56, 133,  56,  66, 135, 132,  56,\n",
       "         66, 133,  56,  66, 135, 132,  56,  66, 133,  56,  66, 135, 132,\n",
       "         56,  66, 133,  56,  66, 135, 132,  56,  66, 133,  56,  66, 135,\n",
       "        132,  56,  66, 133,  56,  66, 135, 132,  56,  66, 133,  72, 135,\n",
       "        132,  72,   1,   0]),\n",
       " array([  0, 134, 133,  75,  72, 135, 132,  75, 133,  68,  75,  56,  44,\n",
       "        135, 132,  68,  75, 133,  68,  75, 135, 132,  56,  68,  72, 133,\n",
       "         68,  72,  56,  60,  63,  77, 135, 132,  68,  72, 133,  68,  72,\n",
       "         80, 135, 132,  56, 133,  56, 135, 132,  68,  72,  80, 135,  44,\n",
       "         56,  60,  63,  75,  77, 133,  56,  44, 135, 132,  56, 139,  44,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  56,  63, 135, 132,  56, 133,  66,  56, 135, 132,\n",
       "         66, 133,  66, 135, 132,  56, 133,  56,  68, 135, 132,  63,  66,\n",
       "        133,  63,  66, 135, 132,  56,  63,  66,  68, 133,  44,  56,  59,\n",
       "         66, 135, 132,  44,  56,  59,  66, 135, 133,  56,  78, 135, 132,\n",
       "         56, 137,  78,   1,   0]),\n",
       " array([  0, 134, 133,  75,  56, 136, 132,  75, 133,  75,  60,  63,  75,\n",
       "        135, 132,  75,  56, 133,  56,  78, 135, 132,  56,  60,  63,  75,\n",
       "         78, 133,  73,  47,  59,  66, 135, 132,  73, 133,  73, 135, 132,\n",
       "         47,  59,  73, 135, 133,  71,  59, 135, 132,  71, 135,  59, 135,\n",
       "         66,   1,   0]),\n",
       " array([  0, 134, 133,  71, 135, 132,  71, 133,  61,  73, 135, 132,  61,\n",
       "         73, 133,  75, 135, 132,  75, 133,  61,  73, 135, 132,  61,  73,\n",
       "        133,  52,  64,  71,  78, 136, 132,  52,  64,  71,  78, 135, 133,\n",
       "         76, 135, 132,  76,   1,   0]),\n",
       " array([  0, 134, 133,  76, 135, 132,  76, 133,  54,  78, 135,  61,  80,\n",
       "        136, 132,  54,  61,  78,  80, 133,  66,  80, 135, 132,  66, 133,\n",
       "         83,  56, 135, 132,  83, 133,  83,  63, 135, 132,  80,  83, 136,\n",
       "         56,  63,   1,   0]),\n",
       " array([  0, 134, 133,  59, 135, 132,  59, 133,  59,  54,  61, 135, 132,\n",
       "         59, 135,  54,  61, 133,  54,  61,  54,  61,  66,  69, 135, 132,\n",
       "         54,  61,  54, 133,  54,  63, 135, 132,  54,  61,  63,  66,  69,\n",
       "        133,  51,  61,  63, 135, 132,  51,  61,  63, 135, 133,  59, 135,\n",
       "        132,  59,   1,   0]),\n",
       " array([  0, 134, 133,  56,  60,  63, 136, 132,  56,  60,  63, 133,  56,\n",
       "         60,  63, 135, 132,  56,  60,  63, 133,  66, 135, 132,  66, 133,\n",
       "         59,  47,  61, 135, 132,  59, 133,  59, 135, 132,  47,  59,  61,\n",
       "        135, 133,  59,  71, 135, 132,  59,  71,   1,   0]),\n",
       " array([  0, 134, 133,  59,  71, 136, 132,  59,  71, 133,  61,  73, 135,\n",
       "        132,  61,  73, 133,  61,  73,  61,  73, 135, 132,  61,  73,  61,\n",
       "         73, 133,  52,  59,  71,  78, 135, 132,  52,  59,  71,  78, 133,\n",
       "         52,  59,  71,  78, 135, 132,  52,  59,  71,  78, 135, 133,  59,\n",
       "        135, 132,  59,   1,   0]),\n",
       " array([  0, 134, 133,  52,  59,  64, 135, 132,  52,  59,  64, 133,  42,\n",
       "         54,  66, 135, 132,  42,  54, 133,  42,  54,  68, 136, 132,  42,\n",
       "         54,  66, 135, 133,  44,  56,  71,  83, 136, 132,  44,  56,  71,\n",
       "         83, 133,  83,  59, 136, 132,  68,  83,  59,   1,   0]),\n",
       " array([  0, 135, 133,  83,  54,  42,  83, 135, 132,  83,  54, 133,  54,\n",
       "         78, 136, 132,  54,  78, 133,  61,  54,  78,  80, 135, 132,  61,\n",
       "        133,  61, 135, 132,  54, 135,  42,  61,  78,  80,  83, 133,  59,\n",
       "        135, 132,  59,   1,   0]),\n",
       " array([  0, 134, 133,  59, 135, 132,  59, 133,  44,  56, 136, 132,  44,\n",
       "         56, 133,  44,  56, 135, 132,  44,  56, 133,  44,  56, 135, 132,\n",
       "         44,  56, 133,  44,  56, 135, 132,  44,  56, 135, 133,  63,  56,\n",
       "        135, 132,  63, 135,  56,   1,   0]),\n",
       " array([  0, 134, 133,  63, 135,  66,  56, 135, 132,  66, 133,  66, 135,\n",
       "        132,  63, 133,  63, 135, 132,  56, 133,  56, 135, 132,  63,  66,\n",
       "        133,  63,  66,  71, 135, 132,  56, 133,  56, 135, 132,  56,  63,\n",
       "         66,  71, 133,  56,  60,  63, 135, 132,  56,  60,  63,   1,   0]),\n",
       " array([  0, 134, 133,  56,  60,  63, 135, 132,  56,  60,  63, 133,  56,\n",
       "         60,  63, 135, 132,  56,  60,  63, 133,  56,  60,  63, 136, 132,\n",
       "         56,  60,  63, 133,  56,  60,  63,  56,  60,  63, 135, 132,  56,\n",
       "         60,  63,  56,  60,  63, 133,  56,  60,  63, 135, 132,  56,  60,\n",
       "         63, 135, 133,  56,  60,  63, 135, 132,  56, 135,  60,  63,   1,\n",
       "          0]),\n",
       " array([  0, 134, 133,  56, 135,  66,  60,  63, 135, 132,  66, 133,  66,\n",
       "        136, 132,  56,  60,  63, 133,  56,  60,  63, 135, 132,  66, 133,\n",
       "         66, 135, 132,  56,  60,  63,  66, 133,  63,  66, 135, 132,  63,\n",
       "         66, 133,  71, 135, 132,  71,   1,   0]),\n",
       " array([  0, 134, 133,  44,  56,  72,  75, 135, 132,  44,  56,  72,  75,\n",
       "        133,  44,  56,  72,  75, 135, 132,  44,  56,  72,  75, 133,  44,\n",
       "         56,  72,  75, 135, 132,  44,  56,  72,  75, 133,  44,  56,  72,\n",
       "         75,  78, 135, 132,  44,  56,  72,  75,  78, 133,  44,  56,  72,\n",
       "         75,  78, 135, 132,  44,  56, 133,  44,  56, 135, 132,  44,  56,\n",
       "         72,  75,  78, 135, 133,  56,  72,  75,  44, 135, 132,  56, 135,\n",
       "         72,  75, 139,  44,   1,   0]),\n",
       " array([  0, 134, 133,  56, 135, 132,  56, 133,  56,  78,  72,  75, 135,\n",
       "        132,  56,  78, 133,  78,  56, 135, 132,  78, 133,  78,  63, 135,\n",
       "        132,  78, 133,  78, 135, 132,  56,  78, 133,  56,  78, 135, 132,\n",
       "         56,  63,  72,  75,  78, 135, 133,  56,  60,  63, 135, 132,  56,\n",
       "         60,  63,   1,   0]),\n",
       " array([  0, 134, 133,  56,  60,  63, 135, 132,  56,  60,  63, 133,  56,\n",
       "         60,  63, 135, 132,  56,  60,  63, 133,  56,  60,  63, 135, 132,\n",
       "         56,  60,  63, 133,  56,  60,  63, 135, 132,  56,  60,  63, 133,\n",
       "         56,  60,  63, 135, 132,  56,  60,  63, 133,  56,  60,  63, 135,\n",
       "        132,  56,  60,  63, 135, 133,  56,  44, 135, 132,  56, 140,  44,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  56, 135, 132,  56, 133,  56,  66, 135, 132,  56,\n",
       "         66, 133,  56,  66, 135, 132,  56,  66, 133,  56,  66, 135, 132,\n",
       "         56,  66, 133,  56,  63,  66, 135, 132,  56,  63,  66, 133,  56,\n",
       "         63,  66, 135, 132,  56,  63,  66, 135, 133,  56,  71, 137, 132,\n",
       "         56, 135,  71,   1,   0]),\n",
       " array([  0, 134, 133,  75,  60,  63, 136, 132,  75, 133,  56,  75, 135,\n",
       "        132,  56,  60,  63,  75, 133,  58, 135, 132,  58, 133,  47,  59,\n",
       "         66,  71,  75, 136, 132,  47,  59,  66,  71,  75, 135, 133,  71,\n",
       "        135, 132,  71,   1,   0]),\n",
       " array([  0, 134, 133,  71, 136, 132,  71, 133,  73,  61,  68,  73, 135,\n",
       "        132,  73, 135,  61,  68,  73, 133,  73,  71,  76,  64,  68, 135,\n",
       "        132,  73, 135,  71, 133,  71, 135, 132,  71,  76, 133,  76, 135,\n",
       "        132,  76, 135,  64,  68,   1,   0]),\n",
       " array([  0, 134, 133,  76, 135, 132,  76, 133,  54,  61,  70,  78, 135,\n",
       "         80, 135, 132,  80, 133,  80, 135, 132,  54,  61,  70,  78,  80,\n",
       "        133,  83,  56,  63,  66,  75, 136, 132,  83, 135, 133,  59, 135,\n",
       "        132,  56,  59,  63,  66,  75,   1,   0]),\n",
       " array([  0, 134, 133,  54, 135, 132,  54, 133,  54,  61,  66, 135,  68,\n",
       "         80, 135, 132,  68,  80, 133,  68,  80, 135, 132,  54,  61,  66,\n",
       "         68,  80, 133,  68,  80, 135, 132,  68,  80, 133,  71,  83, 135,\n",
       "        132,  71,  83, 135, 133,  59, 135, 132,  59,   1,   0]),\n",
       " array([  0, 134, 133,  56,  44,  72,  75,  80, 137, 132,  56, 133,  56,\n",
       "        135, 132,  44,  56,  72,  75,  80, 133,  47,  59,  66,  71,  75,\n",
       "        136, 132,  47,  59, 135,  66,  71,  75, 133,  59, 135, 132,  59,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  59, 135, 132,  59, 133,  61,  49, 135, 132,  61,\n",
       "        133,  61, 135, 132,  49,  61, 133,  61, 135, 132,  61, 133,  52,\n",
       "         59,  64,  66, 135, 132,  52,  59,  64,  66, 133,  52,  59,  64,\n",
       "         66, 135, 132,  52,  59,  64,  66, 133,  52,  59,  64,  66, 135,\n",
       "        132,  52,  59,  64,  66, 133,  76, 137, 132,  76,   1,   0]),\n",
       " array([  0, 136, 133,  54,  61,  66,  69,  78, 135, 132,  54,  61,  66,\n",
       "         69,  78, 136, 133,  45,  57,  64,  69,  71, 135, 132,  45,  57,\n",
       "         64,  69, 135,  71, 133,  57,  69, 135, 132,  57,  69,   1,   0]),\n",
       " array([  0, 134, 133,  57,  69, 135, 132,  57,  69, 133,  71,  59,  47,\n",
       "        135, 132,  71, 133,  71,  54, 136, 132,  59,  71,  47,  54, 133,\n",
       "         59,  71,  49,  61,  68,  71, 135, 132,  59,  71, 135, 133,  78,\n",
       "        135, 132,  78, 135,  49,  61,  68,  71,   1,   0]),\n",
       " array([  0, 134, 133,  78,  90,  49, 135, 132,  78,  90, 133,  78,  90,\n",
       "         56,  63,  78,  90, 135, 132,  78,  90,  56, 133,  56,  77,  80,\n",
       "         89, 135, 132,  49, 133,  49,  61, 135, 132,  56, 133,  56,  85,\n",
       "         73, 135, 132,  49, 133,  49, 135, 132,  49,  56,  61,  63,  77,\n",
       "         78,  80,  85,  89,  90, 135,  73, 133,  73,  80, 135, 132,  73,\n",
       "         80,   1,   0]),\n",
       " array([  0, 134, 133,  73,  80, 135, 132,  73,  80, 133,  73,  80, 135,\n",
       "        132,  73,  80, 133,  71,  75,  80, 135, 132,  71,  75,  80, 133,\n",
       "         47,  59,  71,  75,  80, 135, 132,  47,  59,  71,  75,  80, 135,\n",
       "        133,  44,  56,  71,  75,  80, 135, 132,  44,  56,  71,  75,  80,\n",
       "        135, 133,  59, 135, 132,  59,   1,   0]),\n",
       " array([  0, 134, 133,  63, 135, 132,  63, 133,  54,  42,  71, 136, 132,\n",
       "         54, 133,  54, 135, 132,  42,  71, 133,  61,  66,  71, 135,  75,\n",
       "        135, 132,  54,  61,  66,  71,  75, 133,  51,  80,  75, 135,  78,\n",
       "        135, 132,  51,  78,  80, 135,  75,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  44,  83,  75, 135,  51,\n",
       "        135,  80, 135, 132,  51, 133,  51,  78, 135, 132,  44,  83, 133,\n",
       "         83,  44, 135, 132,  51,  83, 133,  51,  83, 135, 132,  51,  83,\n",
       "        133,  75,  87, 135, 132,  44,  75,  75,  78,  80,  87,   1,   0]),\n",
       " array([  0, 134, 133,  75,  87, 135, 132,  75,  87, 133,  54,  42,  78,\n",
       "         90, 135,  80,  92, 135, 132,  54, 133,  54,  75,  87, 135, 132,\n",
       "         42,  54, 133,  42,  54,  72,  84, 135, 132,  42,  54,  72,  75,\n",
       "         78,  80,  84,  87,  90,  92, 133,  39,  51,  72,  84, 135, 132,\n",
       "         39,  51, 135,  72,  84, 133,  72,  84, 135, 132,  72, 146,  84,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  56,  72,  75,  80,  44, 137, 132,  56, 133,  56,\n",
       "        135, 132,  72,  75,  80, 133,  60,  63,  72,  75,  80, 136, 132,\n",
       "         56,  60,  63,  72,  75,  80, 135, 133,  78, 135, 132,  44,  78,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  75, 135, 132,  75, 133,  71,  42,  54, 135, 132,\n",
       "         71, 133,  71, 136, 132,  42,  54,  71, 133,  71,  42,  54,  63,\n",
       "         71, 135, 132,  71,  42,  54,  63, 133,  42,  54,  63,  77, 135,\n",
       "        132,  42,  54,  63,  71, 133,  56,  44, 135, 132,  56, 133,  63,\n",
       "        135, 132,  44,  63,  77,   1,   0]),\n",
       " array([  0, 134, 133,  63,  73,  85, 135, 132,  63,  73,  85, 133,  72,\n",
       "         84,  56,  44, 135, 132,  72,  84, 133,  72,  84, 135, 132,  56,\n",
       "        133,  85,  56,  73, 135, 132,  85, 133,  63,  60,  75,  85,  87,\n",
       "        135, 132,  63, 133,  63, 135, 132,  56, 135,  44,  60,  63,  72,\n",
       "         73,  75,  84,  85,  87, 133,  72,  84, 136, 132,  72,  84,   1,\n",
       "          0]),\n",
       " array([  0, 135, 133,  72,  84,  54,  42,  75,  87, 135, 132,  72,  84,\n",
       "        135,  54, 133,  54,  80,  92, 135, 132,  42,  54, 133,  42,  54,\n",
       "        135, 132,  42,  54,  75,  80,  87,  92, 133,  78,  90, 135, 132,\n",
       "         78,  90, 135, 133,  59,  90, 135, 132,  59, 143,  90,   1,   0]),\n",
       " array([  0, 134, 133,  59,  83, 135,  85, 135, 132,  59, 133,  87,  59,\n",
       "        135, 132,  87, 133,  87, 135, 132,  87, 133,  87, 135, 132,  87,\n",
       "        133,  87, 135, 132,  87, 133,  87, 135, 132,  87, 133,  63, 135,\n",
       "        132,  63,  85, 135,  59,  83,   1,   0]),\n",
       " array([  0, 134, 133,  63, 135, 132,  63, 133,  63, 135, 132,  63, 133,\n",
       "         63, 135, 132,  63, 133,  63, 135, 132,  63, 133,  63, 135, 132,\n",
       "         63, 133,  63, 135, 132,  63, 133,  63, 135, 132,  63, 133,  61,\n",
       "         68, 135, 132,  61,  68,   1,   0]),\n",
       " array([  0, 134, 133,  61,  68, 135, 132,  61,  68, 133,  47,  59,  71,\n",
       "        135, 132,  47,  59,  71, 133,  61, 135, 132,  61, 133,  61, 135,\n",
       "        132,  61, 133,  51,  58,  63, 135, 132,  51,  58,  63, 133,  63,\n",
       "        135, 132,  63, 133,  63, 135, 132,  63, 133,  73, 135, 132,  73,\n",
       "          1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71, 135, 132,  71, 133,\n",
       "         73, 135, 132,  73, 133,  73, 135, 132,  73, 133,  75,  63,  70,\n",
       "        135, 132,  75, 133,  75, 135, 132,  75, 133,  75, 135, 132,  75,\n",
       "        133,  73, 135, 132,  73, 137,  63,  70,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71, 135, 132,  71, 133,\n",
       "         73, 135, 132,  73, 133,  73, 135, 132,  73, 133,  75,  63,  70,\n",
       "        135, 132,  75, 133,  75, 135, 132,  75, 133,  75, 135, 132,  75,\n",
       "        133,  73, 135, 132,  73, 137,  63,  70,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71, 135, 132,  71, 133,\n",
       "         73, 135, 132,  73, 133,  73, 135, 132,  73, 133,  73, 135, 132,\n",
       "         73, 133,  75,  63,  70, 135, 132,  75, 133,  75, 135, 132,  75,\n",
       "        133,  73, 135, 132,  63,  70,  73,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71,  59, 135, 132,  71,\n",
       "        133,  73, 135, 132,  73, 133,  73, 135, 132,  59,  73, 133,  75,\n",
       "         63, 135, 132,  75, 133,  75, 135, 132,  75, 133,  75, 135, 132,\n",
       "         75, 133,  73,  61, 135, 132,  73, 135,  61,  63,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71,  59, 135, 132,  71,\n",
       "        133,  71, 135, 132,  59,  71, 133,  61,  73, 136, 132,  61,  73,\n",
       "        133,  63,  75,  51, 135, 132,  63, 133,  63, 135, 132,  63, 133,\n",
       "         63, 132,  75, 133,  66,  78, 135, 132,  63, 135,  51,  66,  78,\n",
       "          1,   0]),\n",
       " array([  0, 135, 133,  68,  80,  54,  61, 135, 132,  68,  80, 133,  68,\n",
       "         80,  68,  80, 135, 132,  68,  80, 135,  54,  61,  68,  80, 133,\n",
       "         71,  83,  56,  75,  87,  44, 135, 132,  71,  83, 135,  56,  75,\n",
       "         87, 135,  44, 133,  73, 135, 132,  73,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135,  54, 135,  61, 135, 132,  73, 133,  66,\n",
       "         83, 135,  70,  73,  80, 136, 132,  66, 133,  66, 135, 132,  83,\n",
       "        133,  63, 135, 132,  54,  61,  63,  66,  70,  73,  80,   1,   0]),\n",
       " array([  0, 134, 133,  63, 135, 132,  63, 133,  63, 135, 132,  63, 133,\n",
       "         63, 135, 132,  63, 133,  63, 135, 132,  63, 133,  63, 135, 132,\n",
       "         63, 133,  63, 135, 132,  63, 133,  63, 135, 132,  63, 133,  73,\n",
       "        135, 132,  73,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71, 135, 132,  71, 133,\n",
       "         73, 135, 132,  73, 133,  73, 135, 132,  73, 133,  63,  70,  73,\n",
       "        135, 132,  63,  70,  73, 133,  63,  70,  73, 135, 132,  63,  70,\n",
       "         73, 133,  63,  70,  73, 135, 132,  63,  70,  73, 133,  73, 135,\n",
       "        132,  73,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71,  59, 135, 132,  71,\n",
       "        133,  71, 135, 132,  59,  71, 133,  73, 135, 132,  73, 133,  75,\n",
       "         63,  70, 135, 132,  75, 133,  75, 135, 132,  75, 133,  75, 135,\n",
       "        132,  75, 133,  73,  61, 135, 132,  63,  70,  73, 135,  61,   1,\n",
       "          0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71,  59, 135, 132,  71,\n",
       "        133,  73, 135, 132,  59,  73, 133,  73, 135, 132,  73, 133,  73,\n",
       "        135, 132,  73, 133,  73,  63,  70, 135, 132,  73, 135,  63,  70,\n",
       "        133,  73, 135, 132,  73,   1,   0]),\n",
       " array([  0, 134, 133,  73, 135, 132,  73, 133,  71, 135, 132,  71, 133,\n",
       "         73, 135, 132,  73, 133,  73, 135, 132,  73, 133,  75,  63,  70,\n",
       "        135, 132,  75, 133,  75, 135, 132,  75, 133,  75, 135, 132,  75,\n",
       "        133,  63,  68, 135, 132,  63,  63,  68, 137,  70,   1,   0]),\n",
       " array([  0, 134, 133,  63,  68, 135, 132,  63,  68, 133,  63,  68,  71,\n",
       "        135, 132,  63,  68,  71, 133,  63,  68,  71, 135, 132,  63,  68,\n",
       "         71, 133,  63,  68,  71, 135, 132,  63,  68,  71, 133,  63,  68,\n",
       "         71, 135, 132,  63,  68,  71, 133,  63,  68,  71, 135, 132,  63,\n",
       "         68,  71, 133,  63,  68,  71, 135, 132,  63,  68,  71, 133,  75,\n",
       "        135, 132,  75,   1,   0]),\n",
       " array([  0, 134, 133,  75, 135, 132,  75, 133,  75, 135, 132,  75, 133,\n",
       "         75, 135, 132,  75, 133,  75, 135, 132,  75, 133,  75, 135, 132,\n",
       "         75, 133,  75, 135, 132,  75, 133,  75, 135, 132,  75, 133,  75,\n",
       "        135, 132,  75,   1,   0]),\n",
       " array([  0, 134, 133,  75, 135, 132,  75, 133,  71, 136, 132,  71, 133,\n",
       "         73, 135, 132,  73, 133,  73, 135, 132,  73, 133,  75,  51, 135,\n",
       "         58, 135,  63, 135, 132,  75, 135,  58,  63, 137,  51,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  61,  66,  73,  80, 136,\n",
       "        132,  61,  66,  73,  80, 133,  61,  66,  73,  80, 135, 132,  61,\n",
       "         66,  73,  80, 133,  71,  83, 135, 132,  71,  83, 133,  56,  75,\n",
       "         87,  44, 136, 132,  56,  75,  87, 139,  44,   1,   0]),\n",
       " array([  0, 134, 133,  63, 135, 132,  63, 133,  63, 135, 132,  63, 133,\n",
       "         61,  49,  56, 136, 132,  61,  49,  56, 133,  61,  49,  56,  61,\n",
       "        135, 132,  61,  49,  56,  61, 133,  51,  58,  61, 135, 132,  51,\n",
       "         58,  61, 135, 133,  63, 135, 132,  63,   1,   0]),\n",
       " array([  0, 134, 133,  63, 135, 132,  63, 133,  63, 135, 132,  63, 133,\n",
       "         63, 135, 132,  63, 133,  63, 135, 132,  63, 133,  63, 135, 132,\n",
       "         63, 133,  63, 136, 132,  63, 133,  63,  73, 135, 132,  63, 135,\n",
       "         73,   1,   0]),\n",
       " array([  0, 135, 133,  73,  54,  42,  73,  78, 135, 132,  73, 135,  54,\n",
       "        133,  54,  80,  61,  66, 135, 132,  54,  80, 133,  54,  80, 135,\n",
       "        132,  42,  54,  61,  66,  73,  78,  80, 133,  51,  75,  79,  39,\n",
       "        136, 132,  51,  75,  79, 133,  59, 135, 132,  39,  59,   1,   0]),\n",
       " array([  0, 134, 133,  44, 135,  56, 135, 132,  56, 133,  56, 135, 132,\n",
       "         56, 133,  56, 135, 132,  56, 133,  56, 135, 132,  56, 133,  56,\n",
       "        135, 132,  56, 133,  79,  56, 135, 132,  79, 133,  56,  60,  63,\n",
       "        135, 132,  44,  56,  56, 135,  60,  63,   1,   0]),\n",
       " array([  0, 134, 133,  56,  73, 135, 132,  56,  73, 133,  78,  56,  44,\n",
       "        135, 132,  78, 133,  78, 135, 132,  56,  78, 133,  56,  63,  78,\n",
       "        135, 132,  56, 133,  56,  80, 135, 132,  63,  78, 133,  78,  63,\n",
       "        135, 132,  78, 133,  78, 135, 132,  56, 133,  44,  56,  60,  63,\n",
       "        135, 132,  44,  44,  56,  60,  63,  63, 135,  78,  80,   1,   0]),\n",
       " array([  0, 134, 133,  44,  56,  60,  63, 135, 132,  44,  56,  60,  63,\n",
       "        133,  44,  56,  60,  63, 135, 132,  44,  56,  60,  63, 133,  44,\n",
       "         56,  60,  63, 136, 132,  44,  56,  60,  63, 133,  44,  56,  60,\n",
       "         63,  44,  56,  60,  63, 135, 132,  44,  56,  60,  63,  44,  56,\n",
       "         60,  63, 133,  44,  56,  60,  63, 135, 132,  44,  56,  60,  63,\n",
       "        135, 133,  63,  56, 135, 132,  63, 135,  56,   1,   0]),\n",
       " array([  0, 134, 133,  63, 135, 132,  63, 133,  56,  63,  66, 135, 132,\n",
       "         56,  63,  66, 133,  56,  63,  66, 135, 132,  56,  63, 133,  56,\n",
       "         63, 135, 132,  56,  63,  66, 133,  56,  63,  66, 135, 132,  56,\n",
       "         63,  66, 133,  56,  63,  66, 135, 132,  56,  63,  66, 135, 133,\n",
       "         56,  73, 135, 132,  56, 138,  73,   1,   0]),\n",
       " array([  0, 134, 133,  56,  68,  75, 135, 132,  56, 133,  56, 135, 132,\n",
       "         68, 133,  68, 135, 132,  56, 133,  56, 135, 132,  56,  68, 133,\n",
       "         44,  68, 135,  51, 135, 132,  44, 133,  44, 135, 132,  51, 133,\n",
       "         44,  56,  72,  75, 135, 132,  44,  44,  56,  72,  75,  75, 137,\n",
       "         68,   1,   0]),\n",
       " array([  0, 134, 133,  56,  72,  75,  44, 135, 132,  56,  72,  75, 133,\n",
       "         72,  78,  63,  56, 135, 132,  72,  78, 133,  72,  78, 135, 132,\n",
       "         63,  72,  78, 133,  78,  63,  72, 135, 132,  78, 133,  78, 135,\n",
       "        132,  44,  56,  63,  72,  78, 133,  71,  78, 135, 132,  71,  78,\n",
       "        135, 133,  56, 135, 132,  56,   1,   0]),\n",
       " array([  0, 134, 133,  56,  60,  63,  44, 136, 132,  56, 133,  56, 135,\n",
       "        132,  56,  60,  63, 133,  56,  60,  63,  66, 135, 132,  56,  60,\n",
       "         63, 133,  56,  60,  63,  68, 135, 132,  44, 133,  44, 135, 132,\n",
       "         44,  56,  60,  63, 135,  66,  68, 133,  56,  72,  44, 135, 132,\n",
       "         56, 135,  72, 139,  44,   1,   0]),\n",
       " array([  0, 134, 133,  56, 135,  72,  78,  60,  63, 135, 132,  72,  78,\n",
       "        133,  72,  78, 135, 132,  56,  72,  78, 133,  72,  78,  56, 135,\n",
       "        132,  60,  63,  72,  78, 133,  72,  60,  63,  78, 135, 132,  72,\n",
       "        133,  72,  80, 135, 132,  56,  60,  63,  72,  78,  80,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  78, 135, 132,  78, 133,\n",
       "         78, 135, 132,  78, 133,  78, 135, 132,  78, 133,  78, 135, 132,\n",
       "         78, 133,  78, 135, 132,  78, 133,  47,  59,  66,  71,  73, 135,\n",
       "        132,  47,  59,  66,  71,  73, 133,  59, 135, 132,  59,   1,   0]),\n",
       " array([  0, 134, 133,  59, 136, 132,  59, 133,  61,  49,  61, 135, 132,\n",
       "         61,  49,  61, 133,  61, 135, 132,  61, 133,  52,  59,  64, 136,\n",
       "        132,  52,  59,  64, 133,  52,  59,  64, 135, 132,  52,  59,  64,\n",
       "        133,  52,  64,  71, 135, 132,  52,  64,  71,   1,   0]),\n",
       " array([  0, 134, 133,  52, 135, 132,  52, 133,  54,  73,  78,  42, 135,\n",
       "        132,  54,  73,  78, 133,  54,  73,  78, 136, 132,  42,  54,  73,\n",
       "         78, 133,  80,  44,  71,  80, 135, 132,  80, 135,  44, 133,  44,\n",
       "        135, 132,  44,  71,  80, 133,  71,  80, 136, 132,  71,  80,   1,\n",
       "          0]),\n",
       " array([  0, 135, 133,  71,  80,  71,  80,  42, 135, 132,  71,  80,  71,\n",
       "        133,  71,  78,  49, 135, 132,  71,  78, 133,  71,  56,  78, 135,\n",
       "        132,  71,  80, 133,  71,  75,  80, 135, 132,  42,  49,  56,  71,\n",
       "         75,  78,  80, 133,  71,  75, 135, 132,  71,  75, 135, 133,  59,\n",
       "         63, 135, 132,  59,  63,   1,   0]),\n",
       " array([  0, 134, 133,  56,  44,  80,  92, 137, 132,  56, 133,  56, 135,\n",
       "        132,  44,  56,  80,  92, 133,  59,  47, 136, 132,  59, 135, 133,\n",
       "         59,  66, 136, 132,  47,  59,  66,   1,   0]),\n",
       " array([  0, 135, 133,  59,  66,  61,  49, 135, 132,  59,  66, 135,  61,\n",
       "        133,  61,  61, 135, 132,  61,  49,  61, 133,  52,  59,  64,  68,\n",
       "        137,  76, 135, 132,  52,  59,  64,  68,  76,   1,   0]),\n",
       " array([  0, 134, 133,  76, 135, 132,  76, 133,  78,  54,  61,  66, 135,\n",
       "        132,  78, 133,  80, 135, 132,  54,  61,  66,  80, 135, 133,  57,\n",
       "         64,  69,  73,  76,  83, 135, 132,  57,  64,  69,  73,  76,  83,\n",
       "        136, 133,  57, 135, 132,  57,   1,   0]),\n",
       " array([  0, 134, 133,  57, 135, 132,  57, 133,  59,  47, 135, 132,  59,\n",
       "        133,  59, 135, 132,  59, 133,  59, 135, 132,  47,  59, 133,  49,\n",
       "         61,  88,  80, 135, 132,  49, 133,  49, 135, 132,  49,  61,  88,\n",
       "        135, 133,  49,  85, 135, 132,  80, 136,  49,  85,   1,   0]),\n",
       " array([  0, 135, 133,  56,  83, 135,  49,  85, 135, 132,  56,  83, 133,\n",
       "         56,  83, 135, 132,  49,  85, 133,  49,  61,  85, 135, 132,  56,\n",
       "        133,  56,  90, 135, 132,  49,  56,  61,  83,  85,  90, 135, 133,\n",
       "         80, 136, 132,  80,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  80,  52,  59,  64, 135,\n",
       "        132,  80, 133,  78,  80, 135, 132,  52,  59,  64,  78,  80, 133,\n",
       "         83,  47,  59, 135, 132,  83, 133,  83, 135, 132,  47,  59,  83,\n",
       "        133,  80,  87,  44, 135, 132,  80,  87, 135, 133,  56, 135, 132,\n",
       "         56, 135,  44,   1,   0]),\n",
       " array([  0, 134, 133,  56, 135, 132,  56, 133,  42,  54,  75,  87, 135,\n",
       "        132,  42,  54, 133,  54,  42, 135, 132,  54, 133,  54,  58,  61,\n",
       "        135, 132,  54, 133,  54, 135, 132,  42,  54,  58,  61, 133,  54,\n",
       "         71,  80, 135, 132,  54,  75, 133,  54,  59,  63, 135,  78, 135,\n",
       "        132,  54,  59,  63,  71,  78,  80,  87,   1,   0]),\n",
       " array([  0, 134, 133,  78, 135, 132,  78, 133,  80,  44, 135,  78, 135,\n",
       "        132,  78, 133,  78, 135, 132,  80, 133,  75,  80,  87, 135, 132,\n",
       "         75,  80,  87, 133,  56,  75,  80,  87, 135, 132,  56, 133,  56,\n",
       "        135, 132,  75,  80,  87, 133,  63,  78,  90, 135, 132,  44,  56,\n",
       "         63,  78,  78,  90,   1,   0]),\n",
       " array([  0, 134, 133,  56,  78,  87, 135, 132,  56,  78,  87, 133,  78,\n",
       "         42,  87, 135,  54, 135,  58,  61, 135, 132,  78, 133,  78,  84,\n",
       "        135, 132,  42,  54,  58,  61,  78,  84,  87, 133,  78,  85, 135,\n",
       "        132,  78,  85, 135, 133,  56, 135, 132,  56,   1,   0]),\n",
       " array([  0, 134, 133,  44,  68,  72,  75,  80, 135,  56, 135, 132,  44,\n",
       "        133,  44, 135, 132,  56,  68,  72,  75, 133,  56,  68,  72,  75,\n",
       "        135, 132,  44, 133,  44, 135, 132,  56, 133,  56, 135, 132,  44,\n",
       "         56,  68,  72,  75, 133,  44,  56,  68,  72,  75, 135, 132,  44,\n",
       "         56,  68,  72,  75, 133,  47, 135, 132,  80, 135,  47,   1,   0]),\n",
       " array([  0, 134, 133,  56, 135, 132,  56, 133,  42,  78,  87, 135,  54,\n",
       "        135, 132,  42, 133,  42, 135, 132,  54, 133,  54, 136, 132,  42,\n",
       "         54,  78,  87, 133,  78,  83,  42,  54, 135, 132,  78,  83, 133,\n",
       "         47, 135, 132,  42,  47,  54,   1,   0]),\n",
       " array([  0, 134, 133,  47, 135, 132,  47, 133,  44,  32,  71,  75,  80,\n",
       "        137, 132,  44,  32, 133,  44, 132,  71,  75,  80, 133,  44,  75,\n",
       "         80,  83,  87,  32, 135, 132,  44, 135,  44, 136,  75,  80,  83,\n",
       "         87, 136,  32,   1,   0]),\n",
       " array([  0, 134, 133,  59,  59, 135, 132,  59,  59, 133,  42,  54, 135,\n",
       "        132,  42,  54, 133,  42,  54, 135, 132,  42,  54, 133,  42,  54,\n",
       "        135, 132,  42,  54, 133,  42,  54,  73,  82, 135, 132,  42,  54,\n",
       "         73,  82, 133,  47,  59,  71,  75,  78, 136, 132,  47,  59,  71,\n",
       "         75,  78, 133,  47, 135, 132,  47,   1,   0]),\n",
       " array([  0, 134, 133,  44,  32,  72,  75,  80, 138, 132,  44, 133,  44,\n",
       "        135, 132,  44, 133,  44, 135, 132,  44, 135,  32,  72,  75,  80,\n",
       "        133,  44,  32,  68, 138, 132,  44, 136,  32, 138,  68,   1,   0]),\n",
       " array([  0, 137, 133,  44, 136,  32, 136, 132,  44, 133,  51, 136, 132,\n",
       "         32, 136,  51,   1,   0]),\n",
       " array([  0, 135, 133,  56, 136,  51, 136, 132,  51,  56, 133,  51,  56,\n",
       "         63,  68,  72,  75,  44, 138, 132,  51,  56,  63,  68,  72,  75,\n",
       "        135,  44,   1,   0]),\n",
       " array([0., 1., 0.]),\n",
       " array([0., 1., 0.]),\n",
       " array([0., 1., 0.])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_longest_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 162)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_convert(t_labels, vocab_size):\n",
    "    # Your vocabulary size\n",
    "    vocab_size = 2400\n",
    "\n",
    "    # Create a tensor to hold the one-hot encoded versions\n",
    "    one_hot_tensor = torch.zeros((*t_labels.shape, vocab_size))\n",
    "\n",
    "    # Iterate over each element of the original tensor\n",
    "    for i in range(t_labels.size(0)):\n",
    "        for j in range(t_labels.size(1)):\n",
    "            # Get the value from the original tensor\n",
    "            value = int(t_labels[i, j])\n",
    "            # One-hot encode the value\n",
    "            one_hot = torch.zeros(vocab_size)\n",
    "            one_hot[value] = 1\n",
    "            # Assign it to the corresponding position in the new tensor\n",
    "            one_hot_tensor[i, j] = one_hot\n",
    "    return one_hot_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([161, 103, 2400])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "loss_fct = CrossEntropyLoss()\n",
    "logits = torch.stack(model_output.logits).transpose(0,1)\n",
    "# logits = torch.nan_to_num(logits, nan=0.0, posinf=5, neginf=-5)\n",
    "print(logits.transpose(0,1).shape)\n",
    "t_labels = torch.tensor(padded_labels)\n",
    "t_labels = t_labels[:,1:]\n",
    "one_hot = one_hot_convert(t_labels, 2400)\n",
    "# generate one hot from t_labels\n",
    "\n",
    "# print(t_labels.shape)\n",
    "loss = loss_fct(logits, one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infs = logits==float('-inf')\n",
    "infs[0][80:86]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3789)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fct(logits, one_hot)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "model.enable_input_require_grads()\n",
    "model.zero_grad()\n",
    "loss = loss_fct(logits, one_hot)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.num_bars = 2\n",
    "# output = tokenizer.batch_decode(np.array(padded_labels),feature_extractor_output=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output['pretty_midi_objects'][0].write(\"mountain_out_sanity_check.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpadded_labels.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/numpy/lib/npyio.py:545\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m--> 545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_array(fid, arr, allow_pickle\u001b[38;5;241m=\u001b[39mallow_pickle,\n\u001b[1;32m    547\u001b[0m                        pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(fix_imports\u001b[38;5;241m=\u001b[39mfix_imports))\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.save(\"padded_labels.npy\", [[1,2,3],4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., 134., 133., ...,   0.,   0.,   0.],\n",
       "       [  0., 134., 133., ...,   0.,   0.,   0.],\n",
       "       [  0., 134., 133., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [  0.,   1.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   1.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   1.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"padded_labels.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
